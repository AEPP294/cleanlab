{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous workflows with Datalab\n",
    "\n",
    "This notebook provides a comprehensive guide on using `Datalab` to perform various data quality checks and analyses. We cover multiple workflows to demonstrate the flexibility and power of `Datalab`, focusing on practical examples that address less commonly shown issue checks. Each section includes step-by-step instructions and code examples to help you implement these workflows on your own datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Null Values in a Dataset\n",
    "\n",
    "In this section, we will demonstrate how to use `Datalab` to identify and visualize null values in a dataset.\n",
    "This tutorial will guide you through loading a dataset, detecting null values, and creating a clear visualization of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset\n",
    "\n",
    "First, we will load the dataset into a Pandas DataFrame. For simplicity, we will use a dataset in TSV (tab-separated values) format.\n",
    "Some care is needed when loading the dataset to ensure that the data is correctly parsed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset as a multi-line string\n",
    "dataset_tsv = \"\"\"\n",
    "Age\tGender\tLocation\tAnnual_Spending\tNumber_of_Transactions\tLast_Purchase_Date\n",
    "56.0\tOther\tRural\t4099.62\t3\t2024-01-03\n",
    "NaN\tFemale\tRural\t6421.16\t5\tNaT\n",
    "46.0\tMale\tSuburban\t5436.55\t3\t2024-02-26\n",
    "32.0\tFemale\tRural\t4046.66\t3\t2024-03-23\n",
    "60.0\tFemale\tSuburban\t3467.67\t6\t2024-03-01\n",
    "25.0\tFemale\tSuburban\t4757.37\t4\t2024-01-03\n",
    "38.0\tFemale\tRural\t4199.53\t6\t2024-01-03\n",
    "56.0\tMale\tSuburban\t4991.71\t6\t2024-04-03\n",
    "NaN\n",
    "NaN\tMale\tRural\t4655.82\t1\tNaT\n",
    "40.0\tFemale\tRural\t5584.02\t7\t2024-03-29\n",
    "28.0\tFemale\tUrban\t3102.32\t2\t2024-04-07\n",
    "28.0\tMale\tRural\t6637.99\t11\t2024-04-08\n",
    "NaN\tMale\tUrban\t9167.47\t4\t2024-01-02\n",
    "NaN\tMale\tRural\t6790.46\t3\tNaT\n",
    "NaN\tOther\tRural\t5327.96\t8\t2024-01-03\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(StringIO(dataset_tsv), sep='\\t', parse_dates=[\"Last_Purchase_Date\"])\n",
    "\n",
    "# Display the original DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Encode Categorical Values\n",
    "\n",
    "The features argument in find_issues generally works with a numerical array.\n",
    "Therefore, we need to encode any categorical values numerically. A common workflow is to encode categorical values in the dataset before passing it to the find_issues method.\n",
    "However, some encoding strategies may lose the original null values.\n",
    "\n",
    "Here's a strategy to encode categorical columns while keeping the original DataFrame structure intact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to encode categorical columns\n",
    "def encode_categorical_columns(df, columns, drop=True, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    for column in columns:\n",
    "        # Drop NaN values or replace them with a placeholder\n",
    "        categories = df[column].dropna().unique()\n",
    "        \n",
    "        # Create a mapping from categories to numbers\n",
    "        category_to_number = {category: idx for idx, category in enumerate(categories)}\n",
    "        \n",
    "        # Apply the mapping to the column\n",
    "        df[column + '_encoded'] = df[column].map(category_to_number)\n",
    "\n",
    "    if drop:\n",
    "        df = df.drop(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Encode the categorical columns\n",
    "columns_to_encode = [\"Gender\", \"Location\"]\n",
    "encoded_df = encode_categorical_columns(df, columns=columns_to_encode)\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "display(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Datalab\n",
    "\n",
    "Next, we will initialize `Datalab` with the original DataFrame. This will allow us to use the Datalab methods to find all kinds issues in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Datalab class from cleanlab\n",
    "from cleanlab import Datalab\n",
    "\n",
    "# Initialize Datalab with the original DataFrame\n",
    "lab = Datalab(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Detect Null Values\n",
    "We will use the find_issues method from `Datalab` to detect null values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect issues in the dataset, focusing on null values\n",
    "lab.find_issues(features=encoded_df, issue_types={\"null\": {}})\n",
    "\n",
    "# Display the identified issues\n",
    "null_issues = lab.get_issues(\"null\")\n",
    "display(null_issues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sort the Dataset by Null Issues\n",
    "\n",
    "To better understand the impact of null values, we will sort the original DataFrame by the `null_score` from the `null_issues` DataFrame.\n",
    "\n",
    "This score indicates the severity of null issues for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the issues DataFrame by 'null_score' and get the sorted indices\n",
    "sorted_indices = (\n",
    "    null_issues\n",
    "    .sort_values(\"null_score\")\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Sort the original DataFrame based on the sorted indices from the issues DataFrame\n",
    "sorted_df = df.loc[sorted_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (Optional) Visualize the Results\n",
    "\n",
    "Finally, we will create a nicely formatted DataFrame that highlights the null values and the issues detected by `Datalab`.\n",
    "\n",
    "We will use Pandas' styler to add custom styles for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column of separators\n",
    "separator = pd.DataFrame([''] * len(sorted_df), columns=['|'])\n",
    "\n",
    "# Join the sorted DataFrame, separator, and issues DataFrame\n",
    "combined_df = pd.concat([sorted_df, separator, null_issues], axis=1)\n",
    "\n",
    "# Define functions to highlight null values and Datalab columns\n",
    "def highlight_null_values(val):\n",
    "    if pd.isnull(val):\n",
    "        return 'background-color: yellow'\n",
    "    return ''\n",
    "\n",
    "def highlight_datalab_columns(column):\n",
    "    return 'background-color: lightblue'\n",
    "\n",
    "def highlight_is_null_issue(val):\n",
    "    if val:\n",
    "        return 'background-color: orange'\n",
    "    return ''\n",
    "\n",
    "# Apply styles to the combined DataFrame\n",
    "styled_df = (\n",
    "    combined_df\n",
    "    .style.map(highlight_null_values) # Highlight null and NaT values\n",
    "    .map(highlight_datalab_columns, subset=null_issues.columns) # Highlight columns provided by Datalab\n",
    "    .map(highlight_is_null_issue, subset=['is_null_issue']) # Highlight rows with null issues\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Next Steps\n",
    "\n",
    "This section focused on identifying null values, but `Datalab` can detect a variety of other issues such as (near) duplicates, outliers, label issues and more. Explore our additional tutorials to learn more about how Datalab can enhance your data quality workflows.\n",
    "\n",
    "If you want to learn more about the null issue type, you can read about it [here](issue_type_description.html#null-issue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Underperforming Groups in a Dataset\n",
    "\n",
    "In this section, we will demonstrate how to use `Datalab` to identify underperforming groups in a dataset. This tutorial will guide you through generating a synthetic dataset, training a classifier, and identifying groups that are underperforming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate a Synthetic Dataset\n",
    "\n",
    "First, we will generate a synthetic dataset with blobs. This dataset will include some noisy labels in one of the blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data with blobs\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=2, random_state=42, cluster_std=1.0, shuffle=False)\n",
    "\n",
    "# Add noise to the labels\n",
    "n_noisy_labels = 30\n",
    "y[:n_noisy_labels] = np.random.randint(0, 2, n_noisy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train a Classifier and Obtain Predicted Probabilities\n",
    "Next, we will train a classifier using a stacking approach and obtain predicted probabilities for the dataset using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Obtain predicted probabilities using cross-validation\n",
    "clf = LogisticRegression()\n",
    "pred_probs = cross_val_predict(clf, X, y, cv=3, method=\"predict_proba\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (Optional) Cluster the Data\n",
    "\n",
    "To identify underperforming groups, clustering can be useful.\n",
    "In this step, we optionally use KMeans clustering to find clusters within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to use in GridSearchCV for silhouette score\n",
    "def silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, cluster_labels)\n",
    "\n",
    "# Use GridSearchCV to determine the optimal number of clusters\n",
    "param_grid = {\"n_clusters\": range(2, 10)}\n",
    "grid_search = GridSearchCV(KMeans(random_state=0), param_grid, cv=3, scoring=silhouette_scorer)\n",
    "grid_search.fit(X)\n",
    "\n",
    "# Get the best estimator and predict clusters\n",
    "best_kmeans = grid_search.best_estimator_\n",
    "cluster_ids = best_kmeans.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Identify Underperforming Groups with Datalab\n",
    "\n",
    "We will use `Datalab` to find underperforming groups in the dataset based on the predicted probabilities and optionally the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab import Datalab\n",
    "\n",
    "# Initialize Datalab with the dataset\n",
    "lab = Datalab(data={\"X\": X, \"y\": y}, label_name=\"y\", task=\"classification\")\n",
    "\n",
    "# Find issues related to underperforming groups, optionally using cluster_ids\n",
    "lab.find_issues(\n",
    "    # features=X  # Uncomment this line if 'cluster_ids' is not provided to allow Datalab to run clustering automatically.\n",
    "    pred_probs=pred_probs,\n",
    "    issue_types={\n",
    "        \"underperforming_group\": {\n",
    "            \"threshold\": 0.75,          # Set a custom threshold for identifying underperforming groups.\n",
    "                                        # The default threshold is lower, optimized for higher precision (fewer false positives),\n",
    "                                        # but for this toy example, a higher threshold increases sensitivity to underperforming groups.\n",
    "            \"cluster_ids\": cluster_ids  # Optional: Provide cluster IDs if clustering is used.\n",
    "                                        # If not provided, Datalab will automatically run clustering under the hood.\n",
    "                                        # In that case, you need to provide the 'features' array as an additional argument.\n",
    "            },\n",
    "    },\n",
    ")\n",
    "\n",
    "# Collect the identified issues\n",
    "underperforming_group_issues = lab.get_issues(\"underperforming_group\").query(\"is_underperforming_group_issue\")\n",
    "\n",
    "# Display the issues along with given and predicted labels\n",
    "display(underperforming_group_issues.join(pd.DataFrame({\"given_label\": y, \"predicted_label\": pred_probs.argmax(axis=1)})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (Optional) Visualize the Results\n",
    "\n",
    "Finally, we will optionally visualize the dataset, highlighting the underperforming groups identified by `Datalab`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"tab10\")\n",
    "\n",
    "# Highlight the underperforming group (if any issues are detected)\n",
    "if not underperforming_group_issues.empty:    \n",
    "    plt.scatter(\n",
    "        X[underperforming_group_issues.index, 0], X[underperforming_group_issues.index, 1], \n",
    "        s=100, facecolors='none', edgecolors='r', alpha=0.3, label=\"Underperforming Group\", linewidths=2.0\n",
    "    )\n",
    "else:\n",
    "    print(\"No underperforming group issues detected.\")\n",
    "\n",
    "# Add title and legend\n",
    "plt.title(\"Underperforming Groups in the Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about the underperforming group issue type, you can read about it [here](issue_type_description.html#underperforming-group-issue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Data Valuation on a Dataset\n",
    "\n",
    "In this section, we will show how to use `Datalab` to perform data valuation on a dataset. Data valuation helps you understand the importance of each data point, where you can identify valuable and less valuable data points for your machine learning models.\n",
    "\n",
    "We will use a text dataset for this example, but in principle, this can be applied to any dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare the Dataset\n",
    "We will use a subset of the 20 Newsgroups dataset, which is a collection of newsgroup documents suitable for text classification tasks.\n",
    "For demonstration purposes, we'll classify documents from two categories: \"alt.atheism\" and \"sci.space\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a DataFrame with the text data and labels\n",
    "df_text = pd.DataFrame({\"Text\": newsgroups_train.data, \"Label\": newsgroups_train.target})\n",
    "df_text[\"Label\"] = df_text[\"Label\"].map({i: category for (i, category) in enumerate(newsgroups_train.target_names)})\n",
    "\n",
    "# Display the first few samples\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorize the Text Data\n",
    "We will use a `TfidfVectorizer` to convert the text data into a numerical format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into a feature matrix\n",
    "X_vectorized = vectorizer.fit_transform(df_text[\"Text\"])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "X = X_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform Data Valuation with Datalab\n",
    "\n",
    "Next, we will initialize `Datalab` and perform data valuation to assess the value of each data point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab import Datalab\n",
    "\n",
    "# Initialize Datalab with the dataset\n",
    "lab = Datalab(data=df_text, label_name=\"Label\", task=\"classification\")\n",
    "\n",
    "# Perform data valuation\n",
    "lab.find_issues(features=X, issue_types={\"data_valuation\": {}})\n",
    "\n",
    "# Collect the identified issues\n",
    "data_valuation_issues = lab.get_issues(\"data_valuation\")\n",
    "\n",
    "# Display the data valuation issues\n",
    "display(data_valuation_issues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Optional) Visualize Data Valuation Scores\n",
    "Finally, we will visualize the data valuation scores using a histogram to understand the distribution of scores across different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data for plotting a histogram\n",
    "plot_data = (\n",
    "    data_valuation_issues\n",
    "    # Optionally, add a 'given_label' column to distinguish between labels in the histogram\n",
    "    .join(pd.DataFrame({\"given_label\": df_text[\"Label\"]}))\n",
    ")\n",
    "\n",
    "# Plot histograms of data valuation scores for each label\n",
    "sns.histplot(\n",
    "    data=plot_data,\n",
    "    hue=\"given_label\",  # Comment out if no labels should be used in the visualization\n",
    "    x=\"data_valuation_score\",\n",
    "    bins=15,\n",
    "    element=\"step\",\n",
    "    multiple=\"stack\",  # Stack histograms for different labels\n",
    ")\n",
    "\n",
    "# Set y-axis to a logarithmic scale for better visualization of wide-ranging counts\n",
    "plt.yscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Data Valuation Scores by Label\")\n",
    "plt.xlabel(\"Data Valuation Score\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about the data valuation issue type, you can read about it [here](issue_type_description.html#data-valuation-issue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerate Multiple Issue Checks with Pre-computed kNN Graphs\n",
    "\n",
    "In this section, we will demonstrate how to use pre-computed k-nearest neighbors (kNN) graphs to accelerate the identification of multiple issues in your dataset using `Datalab`. This method leverages the power of kNN graphs to efficiently find and analyze data issues.\n",
    "\n",
    "While we use a toy dataset for demonstration, these steps can be applied to any dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare Your Dataset\n",
    "\n",
    "First, load your dataset. For this example, we'll generate a synthetic dataset, but you should replace this with your own dataset loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Replace this section with your own dataset loading\n",
    "# For demonstration, we create a synthetic classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=5,\n",
    "    n_informative=5,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    flip_y=0.02,\n",
    "    class_sep=2.0,\n",
    "    shuffle=False,\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "# Example: Add a duplicate example to the dataset\n",
    "X[-1] = X[-2] + np.random.rand(5) * 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute kNN Graph\n",
    "We will compute the kNN graph using FAISS, a library for efficient similarity search. This step involves creating a kNN graph that represents the nearest neighbors for each point in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Faiss uses single precision, so we need to convert the data type\n",
    "X_faiss = np.float32(X)\n",
    "\n",
    "# Normalize the vectors for inner product similarity (effectively cosine similarity)\n",
    "faiss.normalize_L2(X_faiss)\n",
    "\n",
    "# Build the index using FAISS\n",
    "index = faiss.index_factory(X_faiss.shape[1], \"HNSW32,Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# Add the dataset to the index\n",
    "index.add(X_faiss)\n",
    "\n",
    "# Perform the search to find k-nearest neighbors\n",
    "k = 10  # Number of neighbors to consider\n",
    "D, I = index.search(X_faiss, k + 1)  # Include the point itself during search\n",
    "\n",
    "# Remove the first column (self-distances)\n",
    "D, I = D[:, 1:], I[:, 1:]\n",
    "\n",
    "# Convert cosine similarity to cosine distance\n",
    "np.clip(1 - D, a_min=0, a_max=None, out=D)\n",
    "\n",
    "# Create the kNN graph\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_knn_graph(distances: np.ndarray, indices: np.ndarray) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Create a K-nearest neighbors (KNN) graph in CSR format from provided distances and indices.\n",
    "\n",
    "    Parameters:\n",
    "    distances (np.ndarray): 2D array of shape (n_samples, n_neighbors) containing distances to nearest neighbors.\n",
    "    indices (np.ndarray): 2D array of shape (n_samples, n_neighbors) containing indices of nearest neighbors.\n",
    "\n",
    "    Returns:\n",
    "    scipy.sparse.csr_matrix: KNN graph in CSR format.\n",
    "    \"\"\"\n",
    "    assert distances.shape == indices.shape, \"distances and indices must have the same shape\"\n",
    "    \n",
    "    n_samples, n_neighbors = distances.shape\n",
    "\n",
    "    # Convert to 1D arrays for CSR matrix creation\n",
    "    indices_1d = indices.ravel()\n",
    "    distances_1d = distances.ravel()\n",
    "    indptr = np.arange(0, n_samples * n_neighbors + 1, n_neighbors)\n",
    "\n",
    "    # Create the CSR matrix\n",
    "    return csr_matrix((distances_1d, indices_1d, indptr), shape=(n_samples, n_samples))\n",
    "\n",
    "knn_graph = create_knn_graph(D, I)\n",
    "\n",
    "# Ensure the kNN graph is sorted by row values\n",
    "from sklearn.neighbors import sort_graph_by_row_values\n",
    "sort_graph_by_row_values(knn_graph, copy=False, warn_when_not_sorted=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a Classifier and Obtain Predicted Probabilities\n",
    "\n",
    "Train a classifier on your dataset and obtain predicted probabilities for the dataset. This step is necessary to identify label-related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Obtain predicted probabilities using cross-validation\n",
    "clf = LogisticRegression()\n",
    "pred_probs = cross_val_predict(clf, X, y, cv=3, method=\"predict_proba\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Identify Data Issues Using Datalab\n",
    "Use the pre-computed kNN graph and predicted probabilities to find issues in the dataset using `Datalab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab import Datalab\n",
    "\n",
    "# Initialize Datalab with the dataset\n",
    "lab = Datalab(data={\"X\": X, \"y\": y}, label_name=\"y\", task=\"classification\")\n",
    "\n",
    "# Perform issue detection using the kNN graph and predicted probabilities, when possible\n",
    "lab.find_issues(knn_graph=knn_graph, pred_probs=pred_probs, features=X)\n",
    "\n",
    "# Collect the identified issues and a summary\n",
    "issues = lab.get_issues()\n",
    "issue_summary = lab.get_issue_summary()\n",
    "\n",
    "# Display the issues and summary\n",
    "display(issue_summary)\n",
    "display(issues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "**Creating the kNN Graph:**\n",
    "\n",
    "- Compute the kNN graph using FAISS or another library, ensuring the self-points (points referring to themselves) are omitted from the neighbors.\n",
    "  - Some distance kernels or search algorithms (like those in FAISS) may return negative distances or suffer from numerical instability when comparing\n",
    "    points that are extremely close to each other. This can lead to incorrect results when constructing the kNN graph.\n",
    "  - **Note**: kNN graphs are generally poorly suited for detecting exact duplicates, especially when the number of exact duplicates exceeds the number of requested neighbors. The strengths of this data structure lie in the assumption that data points are similar but not identical, allowing efficient similarity searches and proximity-based analyses.\n",
    "  - If you are comfortable with exploring non-public API functions in the library, you can use the following helper function to ensure that exact duplicate sets are correctly represented in the kNN graph. Please note, this function is not officially supported and is not part of the public API:\n",
    "\n",
    "    ```python\n",
    "    from cleanlab.internal.neighbor.knn_graph import correct_knn_graph\n",
    "\n",
    "    knn_graph = correct_knn_graph(features=X_faiss, knn_graph=knn_graph)\n",
    "    ```\n",
    "- You may need to handle self-points yourself with third-party libraries.\n",
    "- Construct the CSR (Compressed Sparse Row) matrix from the distances and indices arrays.\n",
    "  - `Datalab` can automatically construct a kNN graph from a numerical `features` array if one is not provided, in an accurate and reliable manner.\n",
    "- Sort the kNN graph by row values.\n",
    "\n",
    "When using kNN graphs, it is important to understand their strengths and limitations to apply them effectively in your ML workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect if data is non-iid\n",
    "\n",
    "In this section, we'll show how to userun a non-iid check of your data with `Datalab`.\n",
    "\n",
    "For this demonstration, we'll work with a 2d dataset where the data points are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare the Dataset\n",
    "\n",
    "For simplicity, we'll just work with a numerical feature embeddings that represent the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_dependent(num_samples):\n",
    "    a1, a2, a3 = 0.6, 0.375, -0.975\n",
    "    X = [np.random.normal(1, 1, 2) for _ in range(3)]\n",
    "    X.extend(a1 * X[i-1] + a2 * X[i-2] + a3 * X[i-3] for i in range(3, num_samples))\n",
    "    return np.array(X)\n",
    "\n",
    "X = generate_data_dependent(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Detect Non-IID Issues Using Datalab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab import Datalab\n",
    "\n",
    "# Initialize Datalab with the dataset\n",
    "lab = Datalab(data={\"X\": X})\n",
    "\n",
    "# Perform data valuation\n",
    "lab.find_issues(features=X, issue_types={\"non_iid\": {}})\n",
    "\n",
    "# Collect the identified issues\n",
    "non_iid_issues = lab.get_issues(\"non_iid\")\n",
    "\n",
    "# Display the non-iid issues\n",
    "display(non_iid_issues.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Optional) Visualize the Results\n",
    "\n",
    "Finally, we'll visualize the dataset and highlight the non-iid issues detected by `Datalab`.\n",
    "\n",
    "Note that only the dataset as a whole can be considered to be non-iid, but no individual data point can be considered non-iid.\n",
    "\n",
    "To be compatible with `Datalab`, the point with the lowest non-iid score is assigned the `is_non_iid_issue` flag if the entire dataset\n",
    "is considered non-iid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the non-iid scores\n",
    "non_iid_issues[\"non_iid_score\"].plot()\n",
    "\n",
    "# Highlight the point assigned as a non-iid issue\n",
    "idx = non_iid_issues.query(\"is_non_iid_issue\").index\n",
    "plt.scatter(idx, non_iid_issues.loc[idx,\"non_iid_score\"], color='red', label='Non-iid Issue', s=100)\n",
    "plt.title(\"Non-iid Scores\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Non-iid Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize dataset ordering\n",
    "plt.scatter(X[:, 0], X[:, 1], c=range(len(X)), cmap='coolwarm', s=100)\n",
    "plt.title(\"Dataset with data-dependent ordering\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(label='Sample Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots help visualize the non-iid scores for each data point and the dataset ordering, highlighting potential dependencies and issues.\n",
    "\n",
    "After detecting non-iid issues, you might be interested in quantifying the likelihood that your dataset is non-iid.\n",
    "\n",
    "To check if your data is non-iid, Datalab computes a p-value. A low p-value (close to 0) indicates strong evidence against the null hypothesis that the data is iid, suggesting significant dependencies or variations in\n",
    "distribution across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p-value:\", lab.get_info(\"non_iid\")[\"p-value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be interested in [this page to learn more about the non-iid issue type](issue_type_description.html#non-iid-issue)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
