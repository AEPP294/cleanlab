{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TensorFlow, Keras, and Datalab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 5-minute quickstart tutorial, we use cleanlab to find potential label errors in a text classification dataset of [IMDB movie reviews](https://ai.stanford.edu/~amaas/data/sentiment/). This dataset contains 50,000 text reviews, each labeled with a binary sentiment polarity label indicating whether the review is positive (1) or negative (0). cleanlab will shortlist _hundreds_ of examples that confuse our ML model the most; many of which are potential label errors, edge cases, or otherwise ambiguous examples.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Build a simple TensorFlow & Keras neural network and wrap it with cleanlab's `KerasWrapperSequential`. This wrapper class makes *any* Keras/Tensorflow model compatible with scikit-learn and compute out-of-sample preddicted probabilites\n",
    "\n",
    "- Use `Datalab` to identify various issues in the text dataset including label issues and near duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Quickstart\n",
    "<br/>\n",
    "    \n",
    "Already have (out-of-sample) `pred_probs` from a model trained on an existing set of labels? Maybe you have some numeric `features` as well? Run the code below to find any potential label errors in your dataset.\n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">  \n",
    "    \n",
    "```ipython3 \n",
    "from cleanlab import Datalab\n",
    "\n",
    "lab = Datalab(data=your_dataset, label_name=\"column_name_of_labels\")\n",
    "lab.find_issues(pred_probs=your_pred_probs, features=your_features)\n",
    "\n",
    "lab.report()\n",
    "lab.get_issues()\n",
    "```\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install required dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install sklearn tensorflow tensorflow-datasets\n",
    "!pip install cleanlab[datalab]\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install git+https://github.com/cleanlab/cleanlab.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs.cleanlab.ai).\n",
    "# If running on Colab, may want to use GPU (select: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "# Package versions we used: tensorflow==2.9.1 scikit-learn==1.2.0 tensorflow_datasets==4.5.2\n",
    "\n",
    "dependencies = [\"cleanlab\", \"sklearn\", \"tensorflow\", \"tensorflow_datasets\", \"datasets\"]\n",
    "\n",
    "# Supress outputs that may appear if tensorflow happens to be improperly installed: \n",
    "import os \n",
    "import logging \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # suppress tensorflow log output \n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL) \n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string \n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score, log_loss \n",
    "from sklearn.model_selection import cross_val_predict \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers \n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "from cleanlab.models.keras import KerasWrapperSequential\n",
    "from cleanlab.count import estimate_cv_predicted_probabilities\n",
    "from cleanlab import Datalab\n",
    "\n",
    "SEED = 123456  # for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# This cell is hidden from docs.cleanlab.ai \n",
    "\n",
    "import random \n",
    "import numpy as np \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the IMDb text dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is provided in TensorFlow's Datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "raw_ds = tfds.load(name=\"imdb_reviews\", split=\"train\", batch_size=-1, as_supervised=True)\n",
    "\n",
    "raw_texts, labels = tfds.as_numpy(raw_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(labels))\n",
    "print(f\"Classes: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first example in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Label: 0\n",
      "Example Text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Example Label: {labels[i]}\")\n",
    "print(f\"Example Text: {raw_texts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as two numpy arrays:\n",
    "\n",
    "1. `raw_texts`for the movie reviews in text format,\n",
    "2. `labels` for the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bringing Your Own Data (BYOD)?\n",
    "\n",
    "You can easily replace the above with your own text dataset, and continue with the rest of the tutorial.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to convert the text strings into vectors which are better suited as inputs for neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define a function to preprocess the text data by:\n",
    "\n",
    "1. Converting it to lower case\n",
    "2. Removing the HTML break tags: `<br />`\n",
    "3. Removing any punctuation marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use a `TextVectorization` layer to preprocess, tokenize, and vectorize our text data to a suitabable format for a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=preprocess_text,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting `vectorize_layer` to the text data creates a mapping of each token (i.e. word) to an integer index. Subsequently, we can vectorize our text data in the train and test sets by using this mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.reset_state()\n",
    "vectorize_layer.adapt(raw_texts)\n",
    "\n",
    "vectorized_texts = vectorize_layer(raw_texts).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subsequent neural network models will directly operate on elements of `vectorized_texts` in order to classify reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a classification model and compute out-of-sample predicted probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a simple neural network for classification with TensorFlow and Keras. We will also wrap it with cleanlab's `KerasWrapperSequential` to make it compatible with sklearn. Note: you can wrap *any* existing Keras model this way, by just replacing `keras.Sequential` with `KerasWrapperSequential` in your code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasWrapperSequential(  \n",
    "    [  \n",
    "        tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
    "        layers.Embedding(max_features + 1, 16),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes),\n",
    "        layers.Softmax()\n",
    "    ],  # outputs probability that text belongs to class 1\n",
    "    compile_kwargs= {\n",
    "      \"optimizer\":\"adam\",\n",
    "      \"loss\":tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      \"metrics\":tf.keras.metrics.CategoricalAccuracy(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find potential labeling errors, cleanlab requires a probabilistic prediction from your model for every datapoint. However, these predictions will be overfitted (and thus unreliable) for examples the model was previously trained on. cleanlab is intended to only be used with out-of-sample predicted probabilities, i.e., on examples held out from the model during the training.\n",
    "\n",
    "Here, we show how to obtain the out-of-sample predicted probabilities from our Keras model defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_n_folds = 3  # for efficiency; values like 5 or 10 will generally work better\n",
    "num_epochs = 15  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "521/521 [==============================] - 6s 11ms/step - loss: 0.6605 - categorical_accuracy: 0.5296\n",
      "Epoch 2/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.5220 - categorical_accuracy: 0.4845\n",
      "Epoch 3/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.4102 - categorical_accuracy: 0.4870\n",
      "Epoch 4/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3454 - categorical_accuracy: 0.4917\n",
      "Epoch 5/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3046 - categorical_accuracy: 0.4943\n",
      "Epoch 6/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2742 - categorical_accuracy: 0.4948\n",
      "Epoch 7/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2508 - categorical_accuracy: 0.4953\n",
      "Epoch 8/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2308 - categorical_accuracy: 0.4947\n",
      "Epoch 9/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2136 - categorical_accuracy: 0.4948\n",
      "Epoch 10/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1992 - categorical_accuracy: 0.4961\n",
      "Epoch 11/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1851 - categorical_accuracy: 0.4975\n",
      "Epoch 12/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1719 - categorical_accuracy: 0.4951\n",
      "Epoch 13/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1605 - categorical_accuracy: 0.4962\n",
      "Epoch 14/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1497 - categorical_accuracy: 0.4975\n",
      "Epoch 15/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1410 - categorical_accuracy: 0.4974\n",
      "261/261 [==============================] - 0s 1ms/step\n",
      "Epoch 1/15\n",
      "521/521 [==============================] - 8s 14ms/step - loss: 0.6581 - categorical_accuracy: 0.5079\n",
      "Epoch 2/15\n",
      "521/521 [==============================] - 5s 10ms/step - loss: 0.5198 - categorical_accuracy: 0.4777\n",
      "Epoch 3/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.4103 - categorical_accuracy: 0.4894\n",
      "Epoch 4/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.3456 - categorical_accuracy: 0.4940\n",
      "Epoch 5/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3056 - categorical_accuracy: 0.4924\n",
      "Epoch 6/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2760 - categorical_accuracy: 0.4942\n",
      "Epoch 7/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2529 - categorical_accuracy: 0.4948\n",
      "Epoch 8/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2336 - categorical_accuracy: 0.4957\n",
      "Epoch 9/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2154 - categorical_accuracy: 0.4963\n",
      "Epoch 10/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2001 - categorical_accuracy: 0.4955\n",
      "Epoch 11/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1876 - categorical_accuracy: 0.4965\n",
      "Epoch 12/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1750 - categorical_accuracy: 0.4954\n",
      "Epoch 13/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1652 - categorical_accuracy: 0.4960\n",
      "Epoch 14/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1552 - categorical_accuracy: 0.4964\n",
      "Epoch 15/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1446 - categorical_accuracy: 0.4963\n",
      "261/261 [==============================] - 0s 1ms/step\n",
      "Epoch 1/15\n",
      "521/521 [==============================] - 10s 19ms/step - loss: 0.6582 - categorical_accuracy: 0.5028\n",
      "Epoch 2/15\n",
      "521/521 [==============================] - 7s 13ms/step - loss: 0.5197 - categorical_accuracy: 0.4946\n",
      "Epoch 3/15\n",
      "521/521 [==============================] - 6s 11ms/step - loss: 0.4068 - categorical_accuracy: 0.4872\n",
      "Epoch 4/15\n",
      "521/521 [==============================] - 5s 10ms/step - loss: 0.3428 - categorical_accuracy: 0.4916\n",
      "Epoch 5/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3018 - categorical_accuracy: 0.4949\n",
      "Epoch 6/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2722 - categorical_accuracy: 0.4951\n",
      "Epoch 7/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2487 - categorical_accuracy: 0.4955\n",
      "Epoch 8/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2303 - categorical_accuracy: 0.4963\n",
      "Epoch 9/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2129 - categorical_accuracy: 0.4972\n",
      "Epoch 10/15\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.1981 - categorical_accuracy: 0.4970\n",
      "Epoch 11/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1837 - categorical_accuracy: 0.4966\n",
      "Epoch 12/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1709 - categorical_accuracy: 0.4997\n",
      "Epoch 13/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1607 - categorical_accuracy: 0.4991\n",
      "Epoch 14/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1502 - categorical_accuracy: 0.4991\n",
      "Epoch 15/15\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.1398 - categorical_accuracy: 0.4981\n",
      "261/261 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_probs = estimate_cv_predicted_probabilities(\n",
    "    vectorized_texts,\n",
    "    labels,\n",
    "    clf=model,\n",
    "    cv_n_folds=cv_n_folds,\n",
    "    clf_kwargs={\"epochs\": num_epochs},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use datalab to find issues in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given labels, out-of-sample predicted probabilities and features, cleanlab can quickly help us identify poorly labeled instances in our text data.\n",
    "\n",
    "Here, we use Datalab to find potential errors in our data. Datalab has several ways of loading the data. In this case, we’ll simply wrap the training features and noisy labels in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"texts\": raw_texts, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is need to audit your data is to call `find_issues()`. We can pass in the predicted probabilites obtained above, and also the `vectorized_texts` array as feature embeddings for our dataset so that Datalab can perform several types of issue checks on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding label issues ...\n",
      "Finding outlier issues ...\n",
      "Fitting OOD estimator based on provided features ...\n",
      "Finding near_duplicate issues ...\n",
      "Audit complete. 1741 issues found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "lab = Datalab(data, label_name=\"labels\")\n",
    "lab.find_issues(pred_probs=pred_probs, features=vectorized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the issue checks are done, we can check out more information using the `report` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the different kinds of issues found in the data:\n",
      "\n",
      "    issue_type    score  num_issues\n",
      "         label 0.940320        1492\n",
      "near_duplicate 0.432059         249\n",
      "       outlier 0.595218           0\n",
      "\n",
      "(Note: A lower score indicates a more severe issue across all examples in the dataset.)\n",
      "\n",
      "\n",
      "----------------------- label issues -----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples whose given label is estimated to be potentially incorrect\n",
      "    (e.g. due to annotation error) are flagged as having label issues.\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 1492\n",
      "Overall dataset quality in terms of this issue: : 0.9403\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "       is_label_issue  label_score  given_label  predicted_label\n",
      "22294            True     0.268942            1                0\n",
      "5204             True     0.268942            1                0\n",
      "15079            True     0.268960            1                0\n",
      "21889            True     0.268974            1                0\n",
      "10676            True     0.268998            1                0\n",
      "\n",
      "\n",
      "------------------ near_duplicate issues -------------------\n",
      "\n",
      "About this issue:\n",
      "\tA (near) duplicate issue refers to two or more examples in\n",
      "    a dataset that are extremely similar to each other, relative\n",
      "    to the rest of the dataset.  The examples flagged with this issue\n",
      "    may be exactly duplicated, or lie atypically close together when\n",
      "    represented as vectors (i.e. feature embeddings).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 249\n",
      "Overall dataset quality in terms of this issue: : 0.4321\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "       is_near_duplicate_issue  near_duplicate_score near_duplicate_sets  distance_to_nearest_neighbor\n",
      "21220                     True                   0.0             [22184]                           0.0\n",
      "11892                     True                   0.0             [17362]                           0.0\n",
      "16827                     True                   0.0             [16886]                           0.0\n",
      "16842                     True                   0.0       [24540, 1564]                           0.0\n",
      "16886                     True                   0.0             [16827]                           0.0\n",
      "\n",
      "Additional Information: \n",
      "threshold: 0.09765678639472157\n",
      "\n",
      "\n",
      "---------------------- outlier issues ----------------------\n",
      "\n",
      "About this issue:\n",
      "\tExamples that are very different from the rest of the dataset \n",
      "    (i.e. potentially out-of-distribution or rare/anomalous instances).\n",
      "    \n",
      "\n",
      "Number of examples with this issue: 0\n",
      "Overall dataset quality in terms of this issue: : 0.5952\n",
      "\n",
      "Examples representing most severe instances of this issue:\n",
      "       is_outlier_issue  outlier_score  nearest_neighbor  distance_to_nearest_neighbor\n",
      "799               False       0.525547             14060                      0.586035\n",
      "18010             False       0.527758              5548                      0.617761\n",
      "11571             False       0.528801               672                      0.614867\n",
      "4774              False       0.530336              2255                      0.602391\n",
      "871               False       0.530380              5424                      0.609298\n"
     ]
    }
   ],
   "source": [
    "lab.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the report, we observe that Datalab has identified many label issues in our dataset. We can view the label errors identified by Datalab and the label quality score for each example using the `get_issues` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.730802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.716673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.284272</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.727785</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.530145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_label_issue  label_score  given_label  predicted_label\n",
       "0           False     0.730802            0                0\n",
       "1           False     0.716673            0                0\n",
       "2            True     0.284272            0                1\n",
       "3           False     0.727785            1                1\n",
       "4           False     0.530145            1                1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_issues = lab.get_issues(\"label\")\n",
    "label_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a dataframe containing a label quality score for each example. These numeric scores lie between 0 and 1, where lower scores indicate examples more likely to be mislabeled. The dataframe also contains a boolean column specifying whether or not each example is identified to have a label issue (indicating it is likely mislabeled)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the subset of examples flagged with label issues, and also sort by label quality score to find the indices of the 10 most likely mislabeled examples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_label_issues = label_issues[label_issues[\"is_label_issue\"] == True]\n",
    "lowest_quality_labels = label_issues[\"label_score\"].argsort()[:10].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanlab found 1492 potential label errors in the dataset.\n",
      "Here are indices of the top 10 most likely errors: \n",
      " [22294  5204 15079 21889 10676 11186 15174 10589 21492 18928]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"cleanlab found {len(identified_label_issues)} potential label errors in the dataset.\\n\"\n",
    "    f\"Here are indices of the top 10 most likely errors: \\n {lowest_quality_labels}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some of the most likely label errors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us inspect these datapoints, we define a method to print any example from the dataset. We then display some of the top-ranked label issues identified by `cleanlab`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_as_df(index):\n",
    "    return pd.DataFrame(\n",
    "        {\"texts\": raw_texts[index], \"labels\": labels[index]},\n",
    "        [index]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...incredibly **awful** score...\"\n",
    ">\n",
    "> - \"...**worst** Foley work ever done.\"\n",
    ">\n",
    "> - \"...script is **incomprehensible**...\"\n",
    ">\n",
    "> - \"...editing is just **bizarre**.\"\n",
    ">\n",
    "> - \"...**atrocious** pan and scan...\"\n",
    ">\n",
    "> - \"...**incoherent mess**...\"\n",
    ">\n",
    "> - \"...**amateur** directing there.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22294</th>\n",
       "      <td>b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.&lt;br /&gt;&lt;br /&gt;The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.&lt;br /&gt;&lt;br /&gt;The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.&lt;br /&gt;&lt;br /&gt;The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) &lt;br /&gt;&lt;br /&gt;This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.&lt;br /&gt;&lt;br /&gt;Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.&lt;br /&gt;&lt;br /&gt;Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            texts  \\\n",
       "22294  b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.<br /><br />The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.<br /><br />The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.<br /><br />The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) <br /><br />This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.<br /><br />Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.<br /><br />Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'   \n",
       "\n",
       "       labels  \n",
       "22294       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(22294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...film seems **cheap**.\"\n",
    ">\n",
    "> - \"...unbelievably **bad**...\"\n",
    ">\n",
    "> - \"...cinematography is **badly** lit...\"\n",
    ">\n",
    "> - \"...everything looking **grainy** and **ugly**.\"\n",
    ">\n",
    "> - \"...sound is so **terrible**...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   texts  \\\n",
       "5204  b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'   \n",
       "\n",
       "      labels  \n",
       "5204       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(5204)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...hard to imagine a **boring** shark movie...\"\n",
    ">\n",
    "> - \"**Poor focus** in some scenes made the production seems **amateurish**.\"\n",
    ">\n",
    "> - \"...**do nothing** to take advantage of...\"\n",
    ">\n",
    "> - \"...**far too few** scenes of any depth or variety.\"\n",
    ">\n",
    "> - \"...just **look flat**...no contrast of depth...\"\n",
    ">\n",
    "> - \"...**introspective** and **dull**...constant **disappointment**.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      texts  \\\n",
       "15079  b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'   \n",
       "\n",
       "       labels  \n",
       "15079       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(15079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also obversed from the report above that Datalab has identified some near duplicates in our dataset. We can similarly get more information about it by calling the `get_issues` method and passing in `near_duplicate` as the argument. \n",
    "\n",
    "Then, we can sort the resulting DataFrame by the near duplicate score to see the most severe duplicate issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_near_duplicate_issue</th>\n",
       "      <th>near_duplicate_score</th>\n",
       "      <th>near_duplicate_sets</th>\n",
       "      <th>distance_to_nearest_neighbor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21220</th>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[22184]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11892</th>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17362]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16827</th>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[16886]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16842</th>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[24540, 1564]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16886</th>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[16827]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_near_duplicate_issue  near_duplicate_score near_duplicate_sets  \\\n",
       "21220                     True                   0.0             [22184]   \n",
       "11892                     True                   0.0             [17362]   \n",
       "16827                     True                   0.0             [16886]   \n",
       "16842                     True                   0.0       [24540, 1564]   \n",
       "16886                     True                   0.0             [16827]   \n",
       "\n",
       "       distance_to_nearest_neighbor  \n",
       "21220                           0.0  \n",
       "11892                           0.0  \n",
       "16827                           0.0  \n",
       "16842                           0.0  \n",
       "16886                           0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_issues = lab.get_issues(\"near_duplicate\")\n",
    "duplicate_issues.sort_values(\"near_duplicate_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the text at index 21220 is a near duplicate as the text at index 22184, let's print both of those to compare their similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21220</th>\n",
       "      <td>b'Yes, he is! ...No, not because of Pintilie likes to undress his actors and show publicly their privies. Pintilie IS THE naked \"emperor\" - so to speak...&lt;br /&gt;&lt;br /&gt;It\\'s big time for someone to state the truth. This impostor is a voyeur, a brat locked in an old man\\'s body. His abundance of nude scenes have no artistic legitimacy whatsoever. It is 100% visual perversion: he gets his kicks by making the actors strip in the buff and look at their willies. And if he does this in front of the audience, he might eve get a hard-on! Did you know that, on the set of \"Niki Ardelean\", he used to embarrass poor Coca Bloss, by telling her: \"Oh, Coca, how I wanna f*** you!\"? She is a great lady, very decent and sensitive, and she became unspeakably ashamed - to his petty satisfaction! And, as a worrying alarm signal about the degree of vulgarity and lack of education in Romanian audiences, so many people are still so foolish to declare these visual obscenities \"works of art\"! Will anyone have ever the decency to expose the truth of it all?'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22184</th>\n",
       "      <td>b'Yes, he is! ...No, not because of Pintilie likes to undress his actors and show publicly their privies. Pintilie IS THE naked \"emperor\" - so to speak...&lt;br /&gt;&lt;br /&gt;It\\'s big time for someone to state the truth. This impostor is a voyeur, a brat locked in an old man\\'s body. His abundance of nude scenes have no artistic legitimacy whatsoever. It is 100% visual perversion: he gets his kicks by making the actors strip in the buff and look at their willies. And if he does this in front of the audience, he might eve get a hard-on! Did you know that, on the set of \"Niki Ardelean\", he used to embarrass poor Coca Bloss, by telling her: \"Oh, Coca, how I wanna f*** you!\"? She is a great lady, very decent and sensitive, and she became unspeakably ashamed - to his petty satisfaction! And, as a worrying alarm signal about the degree of vulgarity and lack of education in Romanian audiences, so many people are still so foolish to declare these visual obscenities \"works of art\"! Will anyone have ever the decency to expose the truth of it all?'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       texts  \\\n",
       "21220  b'Yes, he is! ...No, not because of Pintilie likes to undress his actors and show publicly their privies. Pintilie IS THE naked \"emperor\" - so to speak...<br /><br />It\\'s big time for someone to state the truth. This impostor is a voyeur, a brat locked in an old man\\'s body. His abundance of nude scenes have no artistic legitimacy whatsoever. It is 100% visual perversion: he gets his kicks by making the actors strip in the buff and look at their willies. And if he does this in front of the audience, he might eve get a hard-on! Did you know that, on the set of \"Niki Ardelean\", he used to embarrass poor Coca Bloss, by telling her: \"Oh, Coca, how I wanna f*** you!\"? She is a great lady, very decent and sensitive, and she became unspeakably ashamed - to his petty satisfaction! And, as a worrying alarm signal about the degree of vulgarity and lack of education in Romanian audiences, so many people are still so foolish to declare these visual obscenities \"works of art\"! Will anyone have ever the decency to expose the truth of it all?'   \n",
       "22184  b'Yes, he is! ...No, not because of Pintilie likes to undress his actors and show publicly their privies. Pintilie IS THE naked \"emperor\" - so to speak...<br /><br />It\\'s big time for someone to state the truth. This impostor is a voyeur, a brat locked in an old man\\'s body. His abundance of nude scenes have no artistic legitimacy whatsoever. It is 100% visual perversion: he gets his kicks by making the actors strip in the buff and look at their willies. And if he does this in front of the audience, he might eve get a hard-on! Did you know that, on the set of \"Niki Ardelean\", he used to embarrass poor Coca Bloss, by telling her: \"Oh, Coca, how I wanna f*** you!\"? She is a great lady, very decent and sensitive, and she became unspeakably ashamed - to his petty satisfaction! And, as a worrying alarm signal about the degree of vulgarity and lack of education in Romanian audiences, so many people are still so foolish to declare these visual obscenities \"works of art\"! Will anyone have ever the decency to expose the truth of it all?'   \n",
       "\n",
       "       labels  \n",
       "21220       0  \n",
       "22184       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df([21220, 22184])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these two examples are indeed duplicates of one another! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, datalab can efficiently shortlist the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix these label issues or remove ambiguous and duplicated examples from your dataset to obtain an improved, higher quality dataset for training your next ML model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "label_issue_indices = [5204, 22294, 15079]  # check these examples were found in label issues\n",
    "if not all(x in identified_label_issues.index for x in label_issue_indices):\n",
    "    raise Exception(\"Some highlighted examples are missing from identified_label_issues.\")\n",
    "\n",
    "identified_duplicate_issues = duplicate_issues[duplicate_issues[\"is_near_duplicate_issue\"] == True]\n",
    "duplicate_issue_indices = [21220, 22184]  # check these examples were found in duplicates\n",
    "if not all(x in identified_label_issues.index for x in label_issue_indices):\n",
    "    raise Exception(\"Some highlighted examples are missing from identified_duplicate_issues.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text x TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
