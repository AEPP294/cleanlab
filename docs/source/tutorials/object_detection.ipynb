{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d299c1e8",
   "metadata": {},
   "source": [
    "# Finding Label Errors in Object Detection (object detection) Datasets\n",
    "\n",
    "This 5-minute quickstart tutorial demonstrates how to find potential label errors in object detection datasets. In these datasets each example contains a bounding box and a class label surrounding a physical object within an image scene. Using this labeled data, we train a model to predict classes of objects in an image and their physical locations. The example notebook used to train the model is avalible here (link).\n",
    "\n",
    "Example applications of object detection in computer vision include;\n",
    "- Medecine, such as identifying foreign objects in x-rays\n",
    "- Autonomous driving, identifying pedestrians or objects of interest on the road\n",
    "- Agriculture, picking up crops ready for harvest\n",
    "\n",
    "This tutorial uses a subset of the [COCO (Common Objects in Context)](https://cocodataset.org/#home) dataset with 5 classes: car, chair, cup, person, traffic light. The images in the dataset are taken from everyday scenes, and the given annotations come from both official COCO and SAMA.\n",
    "\n",
    "#### Overview of what we we'll do in this tutorial\n",
    "- Score examples based on their overall label quality score using `cleanlab.object_detection.rank.get_label_quality_scores`\n",
    "- Find examples with label issues using `cleanlab.object_detection.filter.find_label_issues`\n",
    "- Visualize examples using `cleanlab.object_detection.rank.visualize`\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Quickstart\n",
    "<br/>\n",
    "    \n",
    "Already have `labels` and `predictions` in the proper format? Just run the code below to find label issues in your object detection dataset.\n",
    "\n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">  \n",
    "    \n",
    "```python\n",
    "\n",
    "from cleanlab.object_detection.rank import find_label_issues\n",
    "from cleanlab.object_detection.filter import get_label_quality_scores\n",
    "\n",
    "# To get boolean vector of label issues for all images\n",
    "is_label_issue = find_label_issues(labels, predictions)\n",
    "\n",
    "# To get label quality scores for all images\n",
    "label_quality_scores = get_label_quality_scores(labels, predictions)\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d552ab9",
   "metadata": {},
   "source": [
    "# 1. Install required dependencies and download data\n",
    "You can use `pip` to install all packages required for this tutorial as follows\n",
    "```\n",
    "!pip install cleanlab\n",
    "!pip insrall matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90449c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ObjectDetectionBenchmarking/tutorial_obj/predictions.pkl'\n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ObjectDetectionBenchmarking/tutorial_obj/labels.pkl'\n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ObjectDetectionBenchmarking/tutorial_obj/example_images.zip' && unzip -q -o example_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8be4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from cleanlab.object_detection.rank import get_label_quality_scores, issues_from_scores, visualize\n",
    "from cleanlab.object_detection.filter import find_label_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506badc",
   "metadata": {},
   "source": [
    "# 2. Format data, labels and model predictions\n",
    "\n",
    "In object detection datasets, each given `label` is a made up of bounding box coordinates and a class label. A model `prediction` is also made up of a bounding box and class label as well as the model confidence in the prediction (i.e. probability the prediction is correct). For both `get_label_quality_scores` and `find_label_issues` cleanlab requires a list of given labels and a list of model predictions for all examples.\n",
    "\n",
    "The expected format of these `labels` and `predictions` is modeled after what common models like mmdet (link) and detectron2 (link) expect. \n",
    "\n",
    "`labels` is a list of dictionaries where `labels[i]` is a dictionary containing key `labels`, which is a list of class labels corresponding key `bboxxes`, which is a numpy array of bounding boxes. Each bounding box in `labels[i]['bboxes']` is in the format is in the format ``[x1,y1,x2,y2]`` where `(x1,y1)` corresponds to the left hand corner of the box and `(x2,y2)` corresponds to the right. An example output of `labels[i]` is below:\n",
    "\n",
    "```\n",
    "{'bboxes': array([[ 26.5 , 215.25,  88.  , 229.75],\n",
    "        [116.5 , 189.57, 166.5 , 215.07],\n",
    "        [241.95, 180.42, 293.32, 225.82]], dtype=float32),\n",
    " 'labels': array([1, 1, 1]), \n",
    " ...\n",
    " 'seg_map': '000000037777.png'} \n",
    "```\n",
    "\n",
    "`predictions` is a list of numpy arrays where `predictions[i]` is of shape `(M,)` where `M` is the number of classes and `predictions[i][m]` is of shape `(L,5)` where `L` is the number of bounding boxes for class `k` and the five columns correspond to ``[x1,y1,x2,y2,pred_prob]`` returned by the model. Here `pred_prob` is the model confidence in the predicted label. Our example uses `K == 5` classes which is why `predictions[0].shape = (5,)`.\n",
    "\n",
    "An example output of `predictions[i]` is below:\n",
    "\n",
    "```\n",
    "array([array([], shape=(0, 5), dtype=float32),\n",
    "       array([[2.41300339e+02, 1.77116043e+02, 2.97747314e+02, 2.28405350e+02,\n",
    "               1.15757108e-01],\n",
    "              [8.42839813e+01, 1.88506973e+02, 1.69366867e+02, 2.28179962e+02,\n",
    "               1.06976844e-01]], dtype=float32)                               ,\n",
    "       array([[2.09936218e+02, 1.22989265e+02, 2.15952652e+02, 1.33577789e+02,\n",
    "               2.66048908e-01],\n",
    "              [3.35775848e+02, 5.53170547e+01, 3.52000000e+02, 7.79105530e+01,\n",
    "               6.99099600e-02]], dtype=float32)                               ,\n",
    "       array([], shape=(0, 5), dtype=float32),\n",
    "       array([], shape=(0, 5), dtype=float32)], dtype=object)\n",
    "```\n",
    "\n",
    "Once you have pred_probs and labels in the appropriate formats, you can find label issues with cleanlab for any object detection dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01daeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"predictions.pkl\", \"rb\"))\n",
    "labels = pickle.load(open(\"labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed7d97",
   "metadata": {},
   "source": [
    "Hereâ€™s what these look like for an example in our synthetic object detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 120\n",
    "predictions[example_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0607aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[example_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db60bb",
   "metadata": {},
   "source": [
    "We can visualize the bounding boxes and labels side by side using `cleanlabs.object_detection.rank.visualize` function. Here the given labels are in red and the predicted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56705562",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'example_images/' + labels[example_idx]['seg_map']\n",
    "\n",
    "visualize(image_path, labels[example_idx], predictions[example_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff923",
   "metadata": {},
   "source": [
    "# 3. Use cleanlab to find label issues\n",
    "Based on the given `labels` and model `predictions` from our trained model, cleanlab can quickly help us find examples that contain label errors in the dataset. In object detection, label errors are annotations that are more likely imperfect.\n",
    "\n",
    "Examples are imperfect when annotators:\n",
    "- overlooked an object (missing annotated bounding box),\n",
    "- chose the wrong class label for an annotated box in the correct location,\n",
    "- imperfectly annotated the location/edges of a bounding box.\n",
    "\n",
    "\n",
    "For cleanlab, any of these annotation errors should lead to an image with a lower label quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cceeefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_show = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_issue_idx = find_label_issues(labels, predictions, return_indices_ranked_by_score=True)\n",
    "label_issue_idx[:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_label_issue = find_label_issues(labels, predictions)\n",
    "is_label_issue[:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b501dc9",
   "metadata": {},
   "source": [
    "## Label quality scores\n",
    "The above example shows how to identify which examples have label quality issues. We can also compute scores for each example in the dataset which estimate our confidence that this example has been correctly labeled. These scores range between 0 and 1 with smaller values indicating examples whose annotation seems more likely to be imprefect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_label_quality_scores(labels, predictions)\n",
    "scores[:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349521e0",
   "metadata": {},
   "source": [
    "We can also use **issues_from_scores()** which returns an array of issue indices sorted from most to least severe who's label quality scores fall below the threshold if one is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_idx = issues_from_scores(scores, threshold=0.5)\n",
    "issue_idx[:num_examples_to_show], scores[issue_idx][:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b8aa0",
   "metadata": {},
   "source": [
    "# 4. Visualize Label Issues\n",
    "Finally, we can use cleanlab functionality to visualize the potential issue errors. Lets view the first 20 images in our test dataset. To make analysis easier we can also input a `class_labels` dictionary into the function to show up as a legend, print out the example's score from `get_label_quality_scores()` and verdict for if the example is an issue or not from `find_label_issues()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class_labels dictionary to pass in and see labels in the legend\n",
    "class_labels = {\"0\": \"car\", \"1\": \"chair\", \"2\": \"cup\", \"3\":\"person\", \"4\": \"traffic light\"}\n",
    "\n",
    "example_predictions = predictions[:num_examples_to_show]\n",
    "example_labels = labels[:num_examples_to_show]\n",
    "issue_scores = scores[:num_examples_to_show]\n",
    "example_is_label_issue = is_label_issue[:num_examples_to_show]\n",
    "for idx, (prediction,label,score,is_issue) in enumerate(zip(example_predictions,example_labels,issue_scores,example_is_label_issue)):\n",
    "    image_path = 'example_images/' + label['seg_map']\n",
    "    print('idx', idx , 'score:', score, 'is issue:', is_issue)\n",
    "    visualize(image_path, label, prediction, class_labels=class_labels, given_label_overlay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcfe05",
   "metadata": {},
   "source": [
    "Here we notice examples where the predictions and labels are more similar have higher quality scores than those that are missmatched, and are less likeley to be marked as issues. The number of boxes is agnostic to the score, consider the three cross country skiiers recieving a similar score to a single skiier in a similar enviornment.\n",
    "\n",
    "On the contrary, images with missing annotated boxes, or boxes in different locations recieve lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce74938",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "assert 2 not in issue_idx\n",
    "assert 6 not in issue_idx\n",
    "assert 13 in issue_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
