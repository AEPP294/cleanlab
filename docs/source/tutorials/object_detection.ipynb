{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d299c1e8",
   "metadata": {},
   "source": [
    "# Finding Label Errors in Object Detection (object detection) Datasets\n",
    "\n",
    "This 5-minute quickstart tutorial demonstrates how to find potential label errors in object detection datasets. In these datasets each example contains a bounding box and a class label surrounding a physical object within an image scene. Using this labeled data, we train a model to predict classes of objects in an image and their physical locations. The example notebook used to train the model is avalible here (link).\n",
    "\n",
    "This tutorial uses a subset of the [COCO (Common Objects in Context)](https://cocodataset.org/#home) dataset with 5 classes: car, chair, cup, person, traffic light. The images in the dataset are taken from everyday scenes, and the given annotations come from both official COCO and SAMA.\n",
    "\n",
    "#### Overview of what we we'll do in this tutorial\n",
    "- Score examples based on their overall label quality score using `cleanlab.object_detection.rank.get_label_quality_scores`\n",
    "- Find examples with label issues using `cleanlab.object_detection.filter.find_label_issues`\n",
    "- Visualize examples using `cleanlab.object_detection.summary.visualize`\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Quickstart\n",
    "<br/>\n",
    "    \n",
    "Already have `labels` and `predictions` in the proper format? Just run the code below to find label issues in your object detection dataset.\n",
    "\n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">  \n",
    "    \n",
    "```python\n",
    "\n",
    "from cleanlab.object_detection.rank import find_label_issues\n",
    "from cleanlab.object_detection.filter import get_label_quality_scores\n",
    "\n",
    "# To get boolean vector of label issues for all images\n",
    "is_label_issue = find_label_issues(labels, predictions)\n",
    "\n",
    "# To get label quality scores for all images\n",
    "label_quality_scores = get_label_quality_scores(labels, predictions)\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d552ab9",
   "metadata": {},
   "source": [
    "# 1. Install required dependencies and download data\n",
    "You can use `pip` to install all packages required for this tutorial as follows\n",
    "```\n",
    "!pip install cleanlab\n",
    "!pip insrall matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90449c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ObjectDetectionBenchmarking/tutorial_obj/predictions.pkl'\n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ObjectDetectionBenchmarking/tutorial_obj/labels.pkl'\n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ObjectDetectionBenchmarking/tutorial_obj/example_images.zip' && unzip -q -o example_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8be4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from cleanlab.object_detection.rank import get_label_quality_scores, issues_from_scores\n",
    "from cleanlab.object_detection.filter import find_label_issues\n",
    "from cleanlab.object_detection.summary import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506badc",
   "metadata": {},
   "source": [
    "# 2. Format data, labels and model predictions\n",
    "\n",
    "In object detection datasets, each given `label` is a made up of bounding box coordinates and a class label. A model `prediction` is also made up of a bounding box and class label as well as the model confidence in the prediction (i.e. probability the prediction is correct). For both `get_label_quality_scores` and `find_label_issues` cleanlab requires a list of given labels and a list of model predictions for all examples.\n",
    "\n",
    "The expected format of these `labels` and `predictions` is modeled after what common models like mmdet and detectron2 expect. \n",
    "\n",
    "The `5` possible class labels in our dataset are: `[car, chair, cup, person, traffic light]`. These classes are zero-indexed and represented with an integer 0,1,...,4.\n",
    "\n",
    "`labels` is a list of dictionaries where `labels[i]` is a dictionary containing key `labels`, which is a list of class labels corresponding key `bboxxes`, which is a numpy array of bounding boxes. Each bounding box in `labels[i]['bboxes']` is in the format is in the format ``[x1,y1,x2,y2]`` where `(x1,y1)` corresponds to the bottom-left corner of the box and `(x2,y2)` corresponds to the top-right. An example output of `labels[i]` is below:\n",
    "\n",
    "```\n",
    "{'bboxes': array([[ 26.5 , 215.25,  88.  , 229.75],\n",
    "        [116.5 , 189.57, 166.5 , 215.07],\n",
    "        [241.95, 180.42, 293.32, 225.82]], dtype=float32),\n",
    " 'labels': array([1, 1, 1]), \n",
    " ...\n",
    " 'seg_map': '000000037777.png'} \n",
    "```\n",
    "\n",
    "`predictions` is a list of numpy arrays where `predictions[i]` is of shape `(M,)` where `M` is the number of classes and `predictions[i][m]` is of shape `(L,5)` where `L` is the number of bounding boxes for class `k` and the five columns correspond to ``[x1,y1,x2,y2,pred_prob]`` returned by the model. Here `pred_prob` is the model confidence in the predicted label. Our example uses `K == 5` classes which is why `predictions[0].shape = (5,)`.\n",
    "\n",
    "An example output of `predictions[i]` is below:\n",
    "\n",
    "```\n",
    "array([array([], shape=(0, 5), dtype=float32),\n",
    "       array([[2.41300339e+02, 1.77116043e+02, 2.97747314e+02, 2.28405350e+02,\n",
    "               1.15757108e-01],\n",
    "              [8.42839813e+01, 1.88506973e+02, 1.69366867e+02, 2.28179962e+02,\n",
    "               1.06976844e-01]], dtype=float32)                               ,\n",
    "       array([[2.09936218e+02, 1.22989265e+02, 2.15952652e+02, 1.33577789e+02,\n",
    "               2.66048908e-01],\n",
    "              [3.35775848e+02, 5.53170547e+01, 3.52000000e+02, 7.79105530e+01,\n",
    "               6.99099600e-02]], dtype=float32)                               ,\n",
    "       array([], shape=(0, 5), dtype=float32),\n",
    "       array([], shape=(0, 5), dtype=float32)], dtype=object)\n",
    "```\n",
    "\n",
    "Once you have pred_probs and labels in the appropriate formats, you can **find label issues with cleanlab for any object detection dataset**!\n",
    "\n",
    "The 5 possible class labels in our dataset are car, chair, cup, person, traffic light. These classes are zero-indexed and represented with an integer 0,1,...,4. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01daeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"predictions.pkl\", \"rb\"))\n",
    "labels = pickle.load(open(\"labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed7d97",
   "metadata": {},
   "source": [
    "Hereâ€™s what these look like for an example in our synthetic object detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 8\n",
    "predictions[example_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0607aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[example_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db60bb",
   "metadata": {},
   "source": [
    "We can visualize the bounding boxes and labels using `cleanlabs.object_detection.summary.visualize` function. Here the given labels are in red and the predicted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56705562",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'example_images/' + labels[example_idx]['seg_map']\n",
    "\n",
    "visualize(image_path, label=labels[example_idx], prediction=predictions[example_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff923",
   "metadata": {},
   "source": [
    "# 3. Use cleanlab to find label issues\n",
    "Based on the given `labels` and model `predictions` from our trained model, cleanlab can quickly help us find examples that contain label errors in the dataset. In object detection, label errors are annotations that are more likely imperfect.\n",
    "\n",
    "Examples are imperfect when annotators:\n",
    "- overlooked an object (missing annotated bounding box),\n",
    "- chose the wrong class label for an annotated box in the correct location,\n",
    "- imperfectly annotated the location/edges of a bounding box.\n",
    "\n",
    "\n",
    "For cleanlab, any of these annotation errors should lead to an image with a lower label quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cceeefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_show = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_issue_idx = find_label_issues(labels, predictions, return_indices_ranked_by_score=True)\n",
    "label_issue_idx[:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_label_issue = find_label_issues(labels, predictions)\n",
    "is_label_issue[:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b501dc9",
   "metadata": {},
   "source": [
    "## Label quality scores\n",
    "The above example shows how to identify which examples have label quality issues. We can also compute scores for each example in the dataset which estimate our confidence that this example has been correctly labeled. These scores range between 0 and 1 with smaller values indicating examples whose annotation seems more likely to be imprefect.\n",
    "\n",
    "Note: This method is useful for prioritizing which images to review if you have too little time to review all of the label issues by reviewing the images with the lowest scores first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_label_quality_scores(labels, predictions)\n",
    "scores[:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349521e0",
   "metadata": {},
   "source": [
    "We can also use **issues_from_scores()** which returns an array of issue indices sorted from most to least severe who's label quality scores fall below the threshold if one is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_idx = issues_from_scores(scores, threshold=0.5)\n",
    "issue_idx[:num_examples_to_show], scores[issue_idx][:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b8aa0",
   "metadata": {},
   "source": [
    "# 4. Visualize Label Issues\n",
    "Finally, we can use cleanlab functionality to visualize the potential issue errors. Lets take a closer look at an example with a low score using cleanlab's `visualize()` function. To make analysis easier we can also input a `class_names` dictionary into the function to show up as a legend and turn off `overlay` to see the given and predicted labels side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd46d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_issue_idx = issue_idx[10]\n",
    "\n",
    "# create a class_labels dictionary to pass in and see labels in the legend\n",
    "class_names = {\"0\": \"car\", \"1\": \"chair\", \"2\": \"cup\", \"3\":\"person\", \"4\": \"traffic light\"}\n",
    "\n",
    "label = labels[example_issue_idx]\n",
    "prediction = predictions[example_issue_idx]\n",
    "score = scores[example_issue_idx]\n",
    "image_path = 'example_images/' + label['seg_map']\n",
    "\n",
    "print(image_path, '| idx', example_issue_idx , '| label quality score:', score, '| is issue: True')\n",
    "visualize(image_path, label=label, prediction=prediction, class_names=class_names, overlay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d7205",
   "metadata": {},
   "source": [
    "As you can see the given label is marked in a solid red color on the image to the left. This is the provided annotation in which issues are detected in. The model predictions, or the `predicted label` is in blue and overlayed on the image on the right. Each bounding box contains a class number in the top corner (left for given annotations, right for predicted) specific to which class that bounding box was marked as. With our provided `class_names` dictionary we can easily identify which classes were marked in the legend to the right of both images.\n",
    "\n",
    "This image has a **low** label quality score and is marked as an error. On closer inspection we notice the annotator missed a car in the back right corner that the model identified. Additionally, the bounding box for the foreground car the woman is in is cut short a bit on the right. \n",
    "\n",
    "Since our model is not perfect, it incorrectly identified the passenger dog as a person however it was still able to identify issues in the scene. Better trained models will lead to better label error detection but you don't need a near perfect model to identify label issues.\n",
    "\n",
    "\n",
    "### Different kinds of label issues identified by ObjectLab\n",
    "Now lets view the first 5 images in our test dataset that are clearly marked as issues and see what other inconsistencies between the `given` and `predicted` label we can spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f82b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_issue_idx = issue_idx[:num_examples_to_show]\n",
    "\n",
    "issue_predictions = [predictions[idx] for idx in example_issue_idx]\n",
    "issue_labels = [labels[idx] for idx in example_issue_idx]\n",
    "issue_scores = scores[example_issue_idx]\n",
    "\n",
    "for idx, (prediction,label,score) in enumerate(zip(issue_predictions,issue_labels,issue_scores)):\n",
    "    image_path = 'example_images/' + label['seg_map']\n",
    "    print(image_path, '| idx', example_issue_idx[idx] , '| label quality score:', score, '| is issue: True')\n",
    "    visualize(image_path, label=label, prediction=prediction, class_names=class_names, overlay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5a521",
   "metadata": {},
   "source": [
    "In each of these these examples there is a clear variety of issues. The corn-on-the-cob has a swapped label where a chair was identified as a person while the monitor picture is missing annotations for a cup and a person working on a chair in the background. The annotator of a kid's birthday party did not individually highlight each kid (in accordance to COCO guidelines only groups with 10 or more objects of the same type can be identified with a single box as a \"crowd\"). Some cups were forgotten at the dinner party and the empty chair was annotated incorrectly. Finally, the chairs are missing from the foreground of the abandoned school bus. \n",
    "\n",
    "All of these examples has low label quality scores to reflect their low annotation quality.\n",
    "\n",
    "### Other uses of visualize\n",
    "The visualize function is also equally able to visualize non-issues, labels, predictions or just the image seperately and save the created images. Lets take a look at the first couple of images in our dataset with this functionality.\n",
    "\n",
    "We can save the image by providing a `save_path` variable as with `idx=0`. Notice how the label quality score is high for this example and it is marked as a non issue. Both the given and predicted labels closeley resemble each other contributing to the high score.\n",
    "\n",
    "For `idx=1`, notice how we are only passing in the given annotations to visualize. Similarily for `idx=2` we are just looking at the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e770d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image_path = 'example_images/' + labels[idx]['seg_map']\n",
    "print(image_path, '| idx', idx , '| label quality score:', scores[idx], '| is issue:', is_label_issue[idx])\n",
    "visualize(image_path, label=labels[idx], prediction=predictions[idx], class_names=class_names, save_path='./example_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e84a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "image_path = 'example_images/' + labels[idx]['seg_map']\n",
    "print(image_path, '| idx', idx , '| label quality score:', scores[idx], '| is issue:', is_label_issue[idx])\n",
    "visualize(image_path, label=labels[idx], class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "image_path = 'example_images/' + labels[idx]['seg_map']\n",
    "visualize(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcfe05",
   "metadata": {},
   "source": [
    "Here we notice examples where the predictions and labels are more similar have higher quality scores than those that are missmatched, and are less likeley to be marked as issues. The number of boxes is agnostic to the score, consider the three cross country skiiers recieving a similar score to a single skiier in a similar enviornment.\n",
    "\n",
    "On the contrary, images with missing annotated boxes, or boxes in different locations recieve lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce74938",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "import numpy as np\n",
    "\n",
    "assert set([60, 48, 1, 46, 30]).issubset(issue_idx)\n",
    "assert 2 not in issue_idx and 0 not in issue_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc4335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
