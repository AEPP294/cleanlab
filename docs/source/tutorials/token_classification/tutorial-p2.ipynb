{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b7f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install cleanlab termcolor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d67688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package versions: \n",
    "# cleanlab==2.0.0 \n",
    "# termcolor==1.1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Token Classification Label Error Detection - Part 2 \n",
    "\n",
    "In this tutorial, we show how you can use cleanlab to find potential label errors in token classification dataset. Here, we use CONLL-2003 which contains 3,449 sentences and 46,400 tokens (after filtering). \n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Identify potential token label issues using cleanlab's `find_label_issues` method. \n",
    "- Rank sentences using cleanlab's `token_classification.rank.get_label_quality_score` method. \n",
    "- TODO: Train a more robust model by removing problematic sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from termcolor import colored \n",
    "from cleanlab.filter import find_label_issues \n",
    "from rank import get_label_quality_scores \n",
    "from utils import * \n",
    "# change to this after token_classification.rank is pushed to the official package \n",
    "# from cleanlab.token_classification.rank import get_label_quality_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75b45",
   "metadata": {},
   "source": [
    "## 2. Get `pred_probs` and `labels` \n",
    "\n",
    "For more information on how to get `pred_probs` and `labels`, see part 1. Recall that we also need the number of word-level tokens for each sentence. Also get their nested list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519cb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dict(np.load('labels.npz')) \n",
    "pred_probs = dict(np.load('pred_probs.npz')) \n",
    "\n",
    "labels_nl = to_nl(labels) \n",
    "pred_probs_nl = to_nl(pred_probs) \n",
    "\n",
    "labels = [label for labels in labels_nl for label in labels] \n",
    "pred_probs = np.array([pred_prob for pred_probs in pred_probs_nl for pred_prob in pred_probs]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3150f",
   "metadata": {},
   "source": [
    "## 3. Use cleanlab to find potential label issues \n",
    "\n",
    "Based on the given labels and out-of-sample predicted probabilities, cleanlab can quickly help us identify label issues in our dataset. Here we request that the indices of the identified label issues be sorted by cleanlab’s self-confidence score, which measures the quality of each given label via the probability assigned to it in our model’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95dc7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanlab found 611 potential label issues. \n",
      "The top 10 most likely label errors:\n",
      "[32203 31970 40344 34547  6727 31531 34553 46388 28663 13979]\n"
     ]
    }
   ],
   "source": [
    "issues = find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence') \n",
    "top = 10 \n",
    "print('Cleanlab found %d potential label issues. ' % len(issues)) \n",
    "print('The top %d most likely label errors:' % top) \n",
    "print(str(issues[:top])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e2963",
   "metadata": {},
   "source": [
    "Note that the indices are in the flattened format. We map them to a tuple `(i, j)`, which corresponds to the `j`'th word-level token of the `i`'th sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecdc474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_tuple = get_mapping(labels_nl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1375d",
   "metadata": {},
   "source": [
    "Let's look at the top 10 examples cleanlab thinks are most likely to be incorrectly labeled. We obtain the sentences from the original file to display the word-level token label issues in context. \n",
    "\n",
    "\\* Uncomment the block below after tutorials are ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639a34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture \n",
    "# !wget https://data.deepai.org/conll2003.zip && mkdir data \n",
    "# !unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f632872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same from tutorial part 1, probably want to collapse this block \n",
    "filepath = 'data/test.txt'\n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = readfile(filepath) \n",
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "\n",
    "maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "given_labels = [mapping(labels, maps) for labels in given_labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f486f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A Reuter consensus survey sees medical equipment group Radiometer reporting largely unchanged earnings when it publishes first half 19996/97 results next \u001b[31mWednesday\u001b[0m.\n",
      "Given label: ORG, suggested label: O\n",
      "\n",
      "2. \u001b[31mLet\u001b[0m's march together,\" Scalfaro, a northerner himself, said.\n",
      "Given label: LOC, suggested label: O\n",
      "\n",
      "3. Scottish \u001b[31mpremier\u001b[0m division after Saturday's matches:\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "4. \u001b[31mBut\u001b[0m 2 27/11/96 5,000 Burma\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "5. \u001b[31m1.\u001b[0m Fc Cologne 16 8 2 6 31 27 26\n",
      "Given label: ORG, suggested label: O\n",
      "\n",
      "6. \u001b[31mNetwork\u001b[0m operators said the draft laws would hold them responsible for copyright infringements in the system and expose them to multi-billion-dollar liabilities.\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "7. -- \u001b[31mBangkok\u001b[0m newsroom (662) 652-0642\n",
      "Given label: MISC, suggested label: LOC\n",
      "\n",
      "8. The lanky former Leeds United defender did not make his England debut until the age of 30 but eventually won 35 caps and was a key member of the \u001b[31m1966\u001b[0m World Cup winning team with his younger brother, Bobby.\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "9. The basket comprises Algeria's Saharan Blend, Indonesia's Minas, Nigeria's Bonny Light, Saudi Arabia's Arabian Light, \u001b[31mDubai\u001b[0m of the Uae, Venezuela's Tia Juana and Mexico's Isthmus.\n",
      "Given label: MISC, suggested label: LOC\n",
      "\n",
      "10. Two ships loaded on the East \u001b[31mCoast\u001b[0m, three waited to load, six were due.\n",
      "Given label: O, suggested label: LOC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "\n",
    "def color_sentence(sentence, word): \n",
    "    start_idx = sentence.index(word) \n",
    "    before, after = sentence[:start_idx], sentence[start_idx + len(word):]\n",
    "    return '%s%s%s' % (before, colored(word, 'red'), after) \n",
    "\n",
    "def print_issue(issue, show_labels=True): \n",
    "    i, j = index_to_tuple[issue] \n",
    "    issue_label = entities[labels[issue]] \n",
    "    predicted_label = entities[np.argmax(pred_probs[issue])] \n",
    "    issue_word = given_words[i][j] \n",
    "    print('%s' % color_sentence(sentences[i], issue_word)) \n",
    "    if show_labels: \n",
    "        print('Given label: %s, suggested label: %s\\n' % (issue_label, predicted_label)) \n",
    "\n",
    "for idx, issue in enumerate(issues[:top]): \n",
    "    print('%d.' % (idx+1), end=' ') \n",
    "    print_issue(issue) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d04902",
   "metadata": {},
   "source": [
    "Let’s zoom into some specific examples from the above: \n",
    "\n",
    "Given label is `ORG` but should be `O`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf4edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Reuter consensus survey sees medical equipment group Radiometer reporting largely unchanged earnings when it publishes first half 19996/97 results next \u001b[31mWednesday\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "print_issue(issues[0], show_labels=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068a45b",
   "metadata": {},
   "source": [
    "Given label is `MISC` but should be `LOC`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b6ecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- \u001b[31mBangkok\u001b[0m newsroom (662) 652-0642\n"
     ]
    }
   ],
   "source": [
    "print_issue(issues[6], show_labels=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a328ca",
   "metadata": {},
   "source": [
    "Given label is `MISC` but should be `LOC`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "557fafe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The basket comprises Algeria's Saharan Blend, Indonesia's Minas, Nigeria's Bonny Light, Saudi Arabia's Arabian Light, \u001b[31mDubai\u001b[0m of the Uae, Venezuela's Tia Juana and Mexico's Isthmus.\n"
     ]
    }
   ],
   "source": [
    "print_issue(issues[8], show_labels=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213b2b2",
   "metadata": {},
   "source": [
    "## 4. Most common word-level token mislabels \n",
    "\n",
    "It may be useful to examine the most common word-level token mislabels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6bbed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Division' is mislabeled 30 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 30 times\n",
      "\n",
      "'Czech' is mislabeled 13 times\n",
      "-----------------------------\n",
      "labeled as LOC, but predicted as MISC 12 times\n",
      "labeled as ORG, but predicted as LOC 1 times\n",
      "\n",
      "'League' is mislabeled 12 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 8 times\n",
      "labeled as ORG, but predicted as MISC 2 times\n",
      "labeled as LOC, but predicted as O 1 times\n",
      "labeled as LOC, but predicted as ORG 1 times\n",
      "\n",
      "'Conference' is mislabeled 10 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 10 times\n",
      "\n",
      "'Hockey' is mislabeled 10 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 5 times\n",
      "labeled as ORG, but predicted as MISC 5 times\n",
      "\n",
      "'National' is mislabeled 9 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 8 times\n",
      "labeled as O, but predicted as MISC 1 times\n",
      "\n",
      "'Alpine' is mislabeled 8 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 8 times\n",
      "\n",
      "'Western' is mislabeled 8 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 7 times\n",
      "labeled as O, but predicted as LOC 1 times\n",
      "\n",
      "'G' is mislabeled 6 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as MISC 6 times\n",
      "\n",
      "'Union' is mislabeled 6 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 6 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [word for words in given_words for word in words]  \n",
    "frequency = frequent_words(issues, words, labels, pred_probs) \n",
    "show_frequent_issues(frequency, entities, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d03fa5",
   "metadata": {},
   "source": [
    "As shown above, many mislabels are between `O` and `MISC`, which are inherently hard to differentiate. Therefore, you can add label/prediction pairs to `exclude`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda2d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Czech' is mislabeled 13 times\n",
      "-----------------------------\n",
      "labeled as LOC, but predicted as MISC 12 times\n",
      "labeled as ORG, but predicted as LOC 1 times\n",
      "\n",
      "'National' is mislabeled 8 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 8 times\n",
      "\n",
      "'I' is mislabeled 6 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as O 5 times\n",
      "labeled as MISC, but predicted as ORG 1 times\n",
      "\n",
      "'Union' is mislabeled 6 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 6 times\n",
      "\n",
      "'Hockey' is mislabeled 5 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 5 times\n",
      "\n",
      "'United' is mislabeled 5 times\n",
      "-----------------------------\n",
      "labeled as LOC, but predicted as ORG 3 times\n",
      "labeled as ORG, but predicted as LOC 2 times\n",
      "\n",
      "'Fe' is mislabeled 4 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as LOC 3 times\n",
      "labeled as LOC, but predicted as ORG 1 times\n",
      "\n",
      "'Wto' is mislabeled 4 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 4 times\n",
      "\n",
      "'League' is mislabeled 4 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as MISC 2 times\n",
      "labeled as LOC, but predicted as O 1 times\n",
      "labeled as LOC, but predicted as ORG 1 times\n",
      "\n",
      "'At' is mislabeled 4 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as ORG 4 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frequency = frequent_words(issues, words, labels, pred_probs, exclude=[(0, 1), (1, 0)]) \n",
    "show_frequent_issues(frequency, entities, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ef843",
   "metadata": {},
   "source": [
    "## 5. Find issue sentences with particular word \n",
    "\n",
    "If you want to examine a specific token, call `search_token` to return a list of sentence indicies which contain the issue sentence. \n",
    "\n",
    "\\* Do you think we should also output the given and predicted labels? If so, the function will take more outputs, but I will imagine this addition will be useful. It also allows users to exclude certain types of mislabels like the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f4e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 354: Standings of \u001b[31mNational\u001b[0m Hockey\n",
      "\n",
      "Sentence 405: Results of \u001b[31mNational\u001b[0m Hockey\n",
      "\n",
      "Sentence 485: \u001b[31mNational\u001b[0m Football League\n",
      "\n",
      "Sentence 510: \u001b[31mNational\u001b[0m Football Conference\n",
      "\n",
      "Sentence 552: Result of \u001b[31mNational\u001b[0m Football\n",
      "\n",
      "Sentence 3302: Results of \u001b[31mNational\u001b[0m Basketball\n",
      "\n",
      "Sentence 3315: Standings of \u001b[31mNational\u001b[0m Hockey\n",
      "\n",
      "Sentence 3367: Results of \u001b[31mNational\u001b[0m Hockey\n",
      "\n",
      "Sentence 3378: Vancouver Canucks star right wing Pavel Bure was suspended for one game by the \u001b[31mNational\u001b[0m Hockey League and fined$ 1,000 Friday for his hit on Buffalo Sabres defenceman Garry Galley on Wednesday.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = 'National' \n",
    "indicies_with_token = search_token(token, issues, index_to_tuple, given_words) \n",
    "\n",
    "for index in indicies_with_token: \n",
    "    print('Sentence %d: %s\\n' % (index, color_sentence(sentences[index], token))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759108b",
   "metadata": {},
   "source": [
    "## 6. Sentence label quality score \n",
    "\n",
    "Cleanlab can analyze every label in the dataset and provide a numerical score for each sentence. The score ranges between 0 and 1: a lower score indicates that the sentence is more likely to contain at least one error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0b5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2133 - score=0.000007\n",
      "A Reuter consensus survey sees medical equipment group Radiometer reporting largely unchanged earnings when it publishes first half 19996/97 results next \u001b[31mWednesday\u001b[0m.\n",
      "Given label: ORG, suggested label: O\n",
      "\n",
      "Sentence 2123 - score=0.000009\n",
      "\u001b[31mLet\u001b[0m's march together,\" Scalfaro, a northerner himself, said.\n",
      "Given label: LOC, suggested label: O\n",
      "\n",
      "Sentence 2770 - score=0.000010\n",
      "Scottish \u001b[31mpremier\u001b[0m division after Saturday's matches:\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "Sentence 2272 - score=0.000013\n",
      "\u001b[31mBut\u001b[0m 2 27/11/96 5,000 Burma\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "Sentence 604 - score=0.000015\n",
      "\u001b[31m1.\u001b[0m Fc Cologne 16 8 2 6 31 27 26\n",
      "Given label: ORG, suggested label: O\n",
      "\n",
      "Sentence 2102 - score=0.000017\n",
      "\u001b[31mNetwork\u001b[0m operators said the draft laws would hold them responsible for copyright infringements in the system and expose them to multi-billion-dollar liabilities.\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "Sentence 2273 - score=0.000021\n",
      "-- \u001b[31mBangkok\u001b[0m newsroom (662) 652-0642\n",
      "Given label: MISC, suggested label: LOC\n",
      "\n",
      "Sentence 3448 - score=0.000022\n",
      "The lanky former Leeds United defender did not make his England debut until the age of 30 but eventually won 35 caps and was a key member of the \u001b[31m1966\u001b[0m World Cup winning team with his younger brother, Bobby.\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "Sentence 1936 - score=0.000023\n",
      "The basket comprises Algeria's Saharan Blend, Indonesia's Minas, Nigeria's Bonny Light, Saudi Arabia's Arabian Light, \u001b[31mDubai\u001b[0m of the Uae, Venezuela's Tia Juana and Mexico's Isthmus.\n",
      "Given label: MISC, suggested label: LOC\n",
      "\n",
      "Sentence 1106 - score=0.000024\n",
      "Two ships loaded on the East \u001b[31mCoast\u001b[0m, three waited to load, six were due.\n",
      "Given label: O, suggested label: LOC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, token_scores = get_label_quality_scores(labels_nl, pred_probs_nl, return_token_info=True) \n",
    "ranking = np.argsort(scores) \n",
    "\n",
    "for r in ranking[:top]: \n",
    "    print('Sentence %d - score=%.6f' % (r, scores[r])) \n",
    "    issue_index = np.argmin(token_scores[r]) \n",
    "    issue_word = given_words[r][issue_index] \n",
    "    print(color_sentence(sentences[r], issue_word)) \n",
    "    given_label = given_labels[r][issue_index] \n",
    "    suggested_label = np.argmax(pred_probs_nl[r][issue_index]) \n",
    "    print('Given label: %s, suggested label: %s\\n' % (entities[given_label], entities[suggested_label])) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
