{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069a3ce3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%capture \n",
    "# !pip uninstall cleanlab -y \n",
    "# !pip install git+https://github.com/ericwang1997/cleanlab.git@token_classification \n",
    "# !pip install termcolor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d67688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package versions: \n",
    "# cleanlab==2.0.0 (ericwang/cleanlab -b token_classification)\n",
    "# termcolor==1.1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Token Classification Label Error Detection - Part 2 \n",
    "\n",
    "In this tutorial, we show how you can use cleanlab to find potential label errors in a NLP token classification dataset. Here, we use CONLL-2003 which contains 20,718 sentences and 301,361 tokens. In a standard NLP token-classification task, the model predicts the entity of each token within each sentence. In this example, the named entity includes miscellaneous `MISC`, location `LOC`, person `PER`, and organization `ORG`. Unnamed entities are labeled as `O`. \n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Identify potential token label issues using cleanlab's `token_classification.filter.find_label_issues` method. \n",
    "- Rank sentences using cleanlab's `token_classification.rank.get_label_quality_score` method. \n",
    "- TODO: (Clean Learning) Train a more robust model by removing problematic sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from termcolor import colored \n",
    "from cleanlab.token_classification.filter import find_label_issues \n",
    "from cleanlab.token_classification.rank import get_label_quality_scores\n",
    "from utils import * \n",
    "# change to this after token_classification.rank is pushed to the official package \n",
    "# from cleanlab.token_classification.rank import get_label_quality_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75b45",
   "metadata": {},
   "source": [
    "## 2. Get `pred_probs` and `labels` \n",
    "\n",
    "`pred_probs` are out-of-sample model-predicted probabilities of the CoNLL-2003 dataset (including training, development, and testing dataset), obtained via cross-validation. To detect potential labels issues, we first get `pred_probs` and `labels`, which are both in nested-list format, such that: \n",
    "\n",
    "- `pred_probs` is a list of `np.arrays`, such that `pred_probs[i]` is the model-predicted probabilities for the tokens in the i'th sentence, and has shape `(N_i, K)`, where `N_i` is the number of word-level tokens of the `i`'th sentence. Each row of the matrix corresponds to a token `t` and contains the model-predicted probabilities that `t` belongs to each possible class, for each of the K classes. The columns must be ordered such that the probabilities correspond to class 0, 1, ..., K-1. \n",
    "        \n",
    "- `labels` is a list of lists, such that `labels[i]` is a list of token labels of the `i`'th sentence. Same format requirements as `cleanlab.rank.get_label_quality_scores`. \n",
    "\n",
    "Here, indicies are a tuple `(i, j)` unless otherwise specified, which refers to the `j`'th word-level token of the `i`'th sentence. Given that each sentence has different number of tokens, we store `pred_probs` and `labels` as `.npz` files, which can be easily converted to dictionaries. Use `read_npz` to retrieve `pred_probs` and `labels` in nested-list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519cb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = read_npz('labels.npz') \n",
    "pred_probs = read_npz('pred_probs.npz') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3150f",
   "metadata": {},
   "source": [
    "## 3. Use cleanlab to find potential label issues \n",
    "\n",
    "Based on the given labels and out-of-sample predicted probabilities, cleanlab can quickly help us identify label issues in our dataset. Here we request that the indices of the identified label issues be sorted by cleanlab’s self-confidence score, which measures the quality of each given label via the probability assigned to it in our model’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95dc7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanlab found 2255 potential label issues. \n",
      "The top 20 most likely label errors:\n",
      "[(2907, 0), (19392, 0), (9962, 4), (8904, 30), (19303, 0), (12918, 0), (9256, 0), (11855, 20), (18392, 4), (20426, 28), (19402, 21), (14744, 15), (19371, 0), (4645, 2), (83, 9), (10331, 3), (9430, 10), (6143, 25), (18367, 0), (12914, 3)]\n"
     ]
    }
   ],
   "source": [
    "issues = find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence') \n",
    "top = 20 \n",
    "print('Cleanlab found %d potential label issues. ' % len(issues)) \n",
    "print('The top %d most likely label errors:' % top) \n",
    "print(str(issues[:top])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e2963",
   "metadata": {},
   "source": [
    "Let's look at the top 20 examples cleanlab thinks are most likely to be incorrectly labeled. We obtain the sentences from the original file to display the word-level token label issues in context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639a34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture \n",
    "# !wget https://data.deepai.org/conll2003.zip && mkdir data \n",
    "# !unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f632872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse this \n",
    "filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = [], [] \n",
    "\n",
    "for filepath in filepaths: \n",
    "    words, label = readfile(filepath) \n",
    "    given_words.extend(words) \n",
    "    given_labels.extend(label)\n",
    "    \n",
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "\n",
    "maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "given_labels = [mapping(labels, maps) for labels in given_labels] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65421a2d",
   "metadata": {},
   "source": [
    "We show the top 20 potential label issues. Given that `O` and `MISC` are hard to distinguish and can sometimes be ambiguous, they are excluded from the examples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f486f849",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \u001b[31mLittle\u001b[0m change from today's weather expected.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "2. \u001b[31mLet\u001b[0m's march together,\" Scalfaro, a northerner himself, said.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "3. 3. Nastja Rysich (\u001b[31mgermany\u001b[0m) 3.75\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "4. The Spla has fought Khartoum's government forces in the south since 1983 for greater autonomy or independence of the mainly Christian and animist region from the Moslem, Arabised \u001b[31mnorth\u001b[0m.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "5. \u001b[31mMayor\u001b[0m Antonio Gonzalez Garcia, of the opposition Revolutionary Workers' Party, said in Wednesday's letter that army troops recently raided several local farms, stole cattle and raped women.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "6. \u001b[31mSpring\u001b[0m Chg Hrw 12pct Chg White Chg\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "7. \" We have seen the photos but for the moment the palace has no comment,\" a spokeswoman for \u001b[31mPrince\u001b[0m Rainier told Reuters.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "8. Danila 28.5 16\u001b[31m/\u001b[0m12 Caribs/ up W224 Mobil.\n",
      "Given label: O, predicted label: LOC\n",
      "\n",
      "9. A Reuter consensus survey sees medical equipment group Radiometer reporting largely unchanged earnings when it publishes first half 19996/97 results next \u001b[31mWednesday\u001b[0m.\n",
      "Given label: ORG, predicted label: O\n",
      "\n",
      "10. Listing London Denoms (K) 1-10-100 Sale Limits \u001b[31mUs\u001b[0m/ Uk/ Jp/ Fr\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "11. Hapoel Haifa 3 \u001b[31mMaccabi\u001b[0m Tel Aviv 1\n",
      "Given label: O, predicted label: ORG\n",
      "\n",
      "12. The revered Roman Catholic nun was admitted to the Calcutta \u001b[31mhospital\u001b[0m a week ago with high fever and severe vomiting.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "13. The embattled Afghan government said last week that the Kabul-Salang highway would be opened on Monday or Tuesday following talks with the Supreme Coordination Council \u001b[31malliance\u001b[0m led by Jumbish-i-Milli movement of powerful opposition warlord General Abdul Rashid Dostum.\n",
      "Given label: ORG, predicted label: O\n",
      "\n",
      "14. \u001b[31mCan\u001b[0m/ U.s. Dollar Exchange Rate: 1.3570\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "15. \u001b[31mBorn\u001b[0m in 1937 in the central province of Anhui, Dai came to Shanghai as a student and remained in the city as a prolific author and teacher of Chinese.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "16. 1990 - British historian Alan John Percivale \u001b[31m(\u001b[0mA.j.p.) Taylor died.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "17. \u001b[31mPace\u001b[0m bowler Ian Harvey claimed three for 81 for Victoria.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "18. But one must not forget that the Osce only has limited powers there,\" said \u001b[31mCotti\u001b[0m, who is also the Swiss foreign minister.\"\n",
      "Given label: O, predicted label: PER\n",
      "\n",
      "19. Specter met Crown Prince Abdullah and Minister of Defence and Aviation Prince \u001b[31mSultan\u001b[0m in Jeddah, Saudi state television and the official Saudi Press Agency reported.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "20. \u001b[31mSporting\u001b[0m his customary bright green outfit, the U.s. champion clocked 10.03 seconds despite damp conditions to take the scalp of Canada's reigning Olympic champion Donovan Bailey, 1992 champion Linford Christie of Britain and American 1984 and 1988 champion Carl Lewis.\n",
      "Given label: ORG, predicted label: O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_issues(issues, labels, pred_probs, given_words, sentences, exclude=[(0, 1), (1, 0)], top=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d04902",
   "metadata": {},
   "source": [
    "More than half of the potential label issues are identified correctly. As shown above, some examples are ambigious and require manual checking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213b2b2",
   "metadata": {},
   "source": [
    "## 4. Most common word-level token mislabels \n",
    "\n",
    "It may be useful to examine the most common word-level token mislabels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a006bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/' is mislabeled 42 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as LOC 36 times\n",
      "labeled as O, but predicted as PER 4 times\n",
      "labeled as O, but predicted as ORG 2 times\n",
      "\n",
      "'Chicago' is mislabeled 27 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as LOC 22 times\n",
      "labeled as LOC, but predicted as ORG 3 times\n",
      "labeled as MISC, but predicted as ORG 2 times\n",
      "\n",
      "'U.s.' is mislabeled 21 times\n",
      "-----------------------------\n",
      "labeled as LOC, but predicted as ORG 8 times\n",
      "labeled as ORG, but predicted as LOC 6 times\n",
      "labeled as LOC, but predicted as O 3 times\n",
      "labeled as LOC, but predicted as MISC 2 times\n",
      "labeled as MISC, but predicted as LOC 1 times\n",
      "labeled as MISC, but predicted as ORG 1 times\n",
      "\n",
      "'Digest' is mislabeled 20 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as ORG 20 times\n",
      "\n",
      "'Press' is mislabeled 20 times\n",
      "-----------------------------\n",
      "labeled as O, but predicted as ORG 20 times\n",
      "\n",
      "'New' is mislabeled 17 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as LOC 13 times\n",
      "labeled as LOC, but predicted as ORG 2 times\n",
      "labeled as O, but predicted as ORG 1 times\n",
      "labeled as MISC, but predicted as LOC 1 times\n",
      "\n",
      "'and' is mislabeled 16 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as O 7 times\n",
      "labeled as O, but predicted as ORG 5 times\n",
      "labeled as O, but predicted as LOC 3 times\n",
      "labeled as MISC, but predicted as ORG 1 times\n",
      "\n",
      "'Philadelphia' is mislabeled 15 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as LOC 14 times\n",
      "labeled as LOC, but predicted as ORG 1 times\n",
      "\n",
      "'Usda' is mislabeled 13 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as LOC 7 times\n",
      "labeled as ORG, but predicted as PER 5 times\n",
      "labeled as ORG, but predicted as MISC 1 times\n",
      "\n",
      "'York' is mislabeled 12 times\n",
      "-----------------------------\n",
      "labeled as ORG, but predicted as LOC 11 times\n",
      "labeled as LOC, but predicted as ORG 1 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "common_token_issues(issues, given_words, labels, pred_probs, entities, exclude=[(0, 1), (1, 0)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ef843",
   "metadata": {},
   "source": [
    "## 5. Find issue sentences with particular word \n",
    "\n",
    "Call `search_token` to examine the token label issues of a specific token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f4e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 471: Soccer - Keane Signs Four-year Contract With Manchester \u001b[31mUnited\u001b[0m.\n",
      "Given label: LOC, predicted label: ORG\n",
      "\n",
      "Sentence 15658: \u001b[31mUnited\u001b[0m Nations 1996-08-29\n",
      "Given label: ORG, predicted label: LOC\n",
      "\n",
      "Sentence 19072: The Humane Society of the \u001b[31mUnited\u001b[0m States estimates that between 500,000 and one million bites are delivered by dogs each year, more than half of which are suffered by children.\n",
      "Given label: LOC, predicted label: ORG\n",
      "\n",
      "Sentence 19104: \u001b[31mUnited\u001b[0m Nations 1996-12-06\n",
      "Given label: ORG, predicted label: LOC\n",
      "\n",
      "Sentence 19879: 1. \u001b[31mUnited\u001b[0m States Iii (Brian Shimer, Randy Jones) one\n",
      "Given label: ORG, predicted label: LOC\n",
      "\n",
      "Sentence 19910: His father Clarence Woolmer represented \u001b[31mUnited\u001b[0m Province, now renamed Uttar Pradesh, in India's Ranji Trophy national championship and captained the state during 1949.\n",
      "Given label: LOC, predicted label: ORG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = 'United' \n",
    "_ = search_token(token, issues, labels, pred_probs, given_words, sentences, entities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759108b",
   "metadata": {},
   "source": [
    "## 6. Sentence label quality score \n",
    "\n",
    "Cleanlab can analyze every label in the dataset and provide a numerical score for each sentence. The score ranges between 0 and 1: a lower score indicates that the sentence is more likely to contain at least one error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0b5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2907: score=0.000001\n",
      "\u001b[31mLittle\u001b[0m change from today's weather expected.\n",
      "Given label: PER, suggested label: O\n",
      "\n",
      "Sentence 19392: score=0.000001\n",
      "\u001b[31mLet\u001b[0m's march together,\" Scalfaro, a northerner himself, said.\n",
      "Given label: LOC, suggested label: O\n",
      "\n",
      "Sentence 9962: score=0.000001\n",
      "3. Nastja Rysich (\u001b[31mgermany\u001b[0m) 3.75\n",
      "Given label: LOC, suggested label: O\n",
      "\n",
      "Sentence 8904: score=0.000002\n",
      "The Spla has fought Khartoum's government forces in the south since 1983 for greater autonomy or independence of the mainly Christian and animist region from the Moslem, Arabised \u001b[31mnorth\u001b[0m.\n",
      "Given label: LOC, suggested label: O\n",
      "\n",
      "Sentence 19303: score=0.000002\n",
      "\u001b[31mAccess\u001b[0m energy futures prices add to daytime gains.\n",
      "Given label: MISC, suggested label: O\n",
      "\n",
      "Sentence 12918: score=0.000002\n",
      "\u001b[31mMayor\u001b[0m Antonio Gonzalez Garcia, of the opposition Revolutionary Workers' Party, said in Wednesday's letter that army troops recently raided several local farms, stole cattle and raped women.\n",
      "Given label: PER, suggested label: O\n",
      "\n",
      "Sentence 9256: score=0.000002\n",
      "\u001b[31mSpring\u001b[0m Chg Hrw 12pct Chg White Chg\n",
      "Given label: LOC, suggested label: O\n",
      "\n",
      "Sentence 11855: score=0.000002\n",
      "\" We have seen the photos but for the moment the palace has no comment,\" a spokeswoman for \u001b[31mPrince\u001b[0m Rainier told Reuters.\n",
      "Given label: PER, suggested label: O\n",
      "\n",
      "Sentence 18392: score=0.000002\n",
      "Danila 28.5 16\u001b[31m/\u001b[0m12 Caribs/ up W224 Mobil.\n",
      "Given label: O, suggested label: LOC\n",
      "\n",
      "Sentence 20426: score=0.000002\n",
      "Albanian coach Astrit Hafizi said on Saturday it was important that his players brush aside the country's short ban by Fifa in order to concentrate on next \u001b[31mSaturday'sWorld\u001b[0m Cup group nine qualifier against Northern Ireland.\n",
      "Given label: MISC, suggested label: O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, token_scores = get_label_quality_scores(labels, pred_probs, return_token_info=True) \n",
    "show_sentence_issues(scores, token_scores, pred_probs, given_words, sentences, given_labels, entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
