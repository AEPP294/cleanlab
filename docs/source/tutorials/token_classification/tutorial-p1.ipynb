{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59db66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install transformers tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20de927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages version: \n",
    "# numpy == 1.23.0 \n",
    "# tqdm == 4.64.0 \n",
    "# transformers == 4.20.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Token Classification Label Error Detection - Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676421",
   "metadata": {},
   "source": [
    "In this tutorial, we show how you can retrieve the model-predictive probabilities and labels from a NLP token-classification dataset. These inputs are used to identify potential label issues in the dataset. \n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Use pre-trained HuggingFace models to get the predictive probabilities \n",
    "- Reduce subword-level tokens to word-level tokens\n",
    "\n",
    "\\* In most NLP literatures, tokens typically refer to words or punctuation marks, while most HuggingFace tokenizers break down longer words into subwords. To avoid confusion, given tokens are referred as \"word-level tokens\", and tokens obtained from the tokenizers as \"subword-level tokens\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0fb4a",
   "metadata": {},
   "source": [
    "Disable `TOKENIZERS_PARALLELISM` if multiple processors exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers tqdm \n",
    "import numpy as np\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from utils import * \n",
    "from tqdm import tqdm \n",
    "\n",
    "import os \n",
    "if os.cpu_count() > 1: \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e32604",
   "metadata": {},
   "source": [
    "## 2. Fetch the CONLL-2003 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced3d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!wget https://data.deepai.org/conll2003.zip && mkdir data \n",
    "!unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cd5a9",
   "metadata": {},
   "source": [
    "CONLL-2003 dataset is in the following format: \n",
    "\n",
    "`-DOCSTART- -X- -X- O` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of first sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "`[empty line]` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of second sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "In our example, we focus on the `ner_tags` (named-entity recognition tags), which include: \n",
    "\n",
    "| `ner_tags` |             Description              |\n",
    "|:----------:|:------------------------------------:|\n",
    "|     `O`    |      Other (not a named entity)      |\n",
    "|   `B-MIS`  | Beginning of a miscellaneous entity  |\n",
    "|   `I-MIS`  |         Miscellaneous entity         |\n",
    "|   `B-PER`  |     Beginning of a person entity     |\n",
    "|   `I-PER`  |            Person entity             |\n",
    "|   `B-ORG`  | Beginning of an organization entity  |\n",
    "|   `I-ORG`  |         Organization entity          |\n",
    "|   `B-LOC`  |    Beginning of a location entity    |\n",
    "|   `I-LOC`  |           Location entity            | \n",
    "\n",
    "For more information, see [here](https://paperswithcode.com/dataset/conll-2003). Here, all-caps words are casted into lowercase except for the first character (eg. `JAPAN` -> `Japan`). This is to discourage the tokenizer from breaking such words into multiple subwords. The `readfile` implementation is adapted from [here](https://github.com/kamalkraj/BERT-NER/blob/dev/run_ner.py#L92). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data/test.txt'\n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = readfile(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148277",
   "metadata": {},
   "source": [
    "`given_words` and `given_labels` are in nested list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d505f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tLabel\tEntity\n",
      "-------------------------------\n",
      "Soccer          0\tO         \n",
      "-               0\tO         \n",
      "Japan           7\tB-LOC     \n",
      "Get             0\tO         \n",
      "Lucky           0\tO         \n",
      "Win             0\tO         \n",
      ",               0\tO         \n",
      "China           3\tB-PER     \n",
      "In              0\tO         \n",
      "Surprise        0\tO         \n",
      "Defeat          0\tO         \n",
      ".               0\tO         \n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "\n",
    "print('Word\\t\\tLabel\\tEntity\\n-------------------------------') \n",
    "for word, label in zip(given_words[i], given_labels[i]): \n",
    "    print('{:14s}{:3d}\\t{:10s}'.format(word, label, entities[label])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8639b",
   "metadata": {},
   "source": [
    "Next, obtain the sentences with some minor pre-processing for readability. Sentences that contain `#` or have length shorter than or equal to one character are removed. The first condition is to ensure that the symbol does not get confused with the same symbol representing subwords after the sentence is being tokenized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f19eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c496b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3449\n",
      "Soccer - Japan Get Lucky Win, China In Surprise Defeat.\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences: %d' % len(sentences)) \n",
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d30f9",
   "metadata": {},
   "source": [
    "## 3. Pre-trained model \n",
    "\n",
    "In this example, we use `dslim/bert-base-NER` as our NLP model and tokenizer. Note that most tokenizers break down sentences into subword-level tokens, which are units smaller than word-level tokens. Since we are only interested in label issues at a word level, we first need to know how each sentence is tokenized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374f0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dslim/bert-base-NER' \n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline(task=\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4c26e",
   "metadata": {},
   "source": [
    "For example, the following sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a87c67b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soccer - Japan Get Lucky Win, China In Surprise Defeat.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4a534",
   "metadata": {},
   "source": [
    "is tokenized into: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e17ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Sur', '##prise', 'De', '##fe', '##at', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer(sentences[0])['input_ids'] \n",
    "tokens = [tokenizer.decode(token) for token in token_ids] \n",
    "print(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d7a62",
   "metadata": {},
   "source": [
    "`[CLS]` and `[SEP]` are special tokens to help the BERT model to identify the start and end of each sentence, and `##` indicates that the token is a subword. We remove them and manually map them to the original tokens afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4a82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Sur', 'prise', 'De', 'fe', 'at', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.replace('#', '') for token in tokens][1:-1] \n",
    "print(tokens) \n",
    "sentence_tokens = [[tokenizer.decode(token) for token in tokenizer(sentence)['input_ids']] for sentence in sentences] \n",
    "sentence_tokens = [[token.replace('#', '') for token in sentence_token][1:-1] for sentence_token in sentence_tokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e1fc2",
   "metadata": {},
   "source": [
    "## 4. Get predictive probabilities \n",
    "\n",
    "In this example, we are more interested in severe types of mislabels, such as `B-LOC` vs. `B-PER`, instead of `B-LOC` vs. `I-LOC`. Therefore, we discard the `B-` and `I-` prefixes, and get the model-predictive probabilities for each subword-level token. Set `merge_entities=False` if you do not wish to merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e199b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 3449/3449 [01:01<00:00, 56.25it/s]\n"
     ]
    }
   ],
   "source": [
    "merge_entities = True \n",
    "\n",
    "if merge_entities: \n",
    "    given_labels = [mapping(labels, maps) for labels in given_labels] \n",
    "    \n",
    "sentence_probs = [get_probs(sentence, pipe, maps=maps) for sentence in tqdm(sentences)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3baf",
   "metadata": {},
   "source": [
    "## 5. Reducing from subword-level to word-level \n",
    "\n",
    "Reduce subword-level tokens to word-level tokens. Here, we show an example of how the reduction is implemented. Consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43366dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tSoccer - Japan Get Lucky Win, China In Surprise Defeat.\n",
      "Given words:\t['Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Surprise', 'Defeat', '.']\n",
      "Subwords:\t['Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Sur', 'prise', 'De', 'fe', 'at', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:\\t' + sentences[0]) \n",
    "print('Given words:\\t' + str(given_words[0])) \n",
    "print('Subwords:\\t' + str(sentence_tokens[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eccd57",
   "metadata": {},
   "source": [
    "The token `Surprise` is tokenized into two subwords `Sur` and `prise`. In this case, we assign the average predictive probabilities of the two subwords to the token. Alternatively, we can take the weighted average, such that the weight for each predictive probability is proportional to the length of its corresponding subword. This is to ensure that longer subwords have heavier weights on the average predictive probabilities, although the benefits are insignificant for most datasets. \n",
    "\n",
    "Each tokenizer tokenizes sentences differently. In some rare cases, a subword may overlap two tokens, resulting in a misalignment in tokenization. For example, consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0565e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tMassachusetts Institute of Technology (MIT)\n",
      "Given words:\t['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')']\n",
      "Subwords:\t['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')']\n"
     ]
    }
   ],
   "source": [
    "demo_sentence = 'Massachusetts Institute of Technology (MIT)' \n",
    "demo_given_words = ['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')'] \n",
    "demo_subwords = ['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')'] \n",
    "\n",
    "print('Sentence:\\t' + demo_sentence) \n",
    "print('Given words:\\t' + str(demo_given_words)) \n",
    "print('Subwords:\\t' + str(demo_subwords)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794f3d0",
   "metadata": {},
   "source": [
    "In this case, we assign the predictive probabilities of `(M` to `(`, and the average predictive probabilities of `(M` and `IT` to `MIT`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3656f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_and_labels = [get_pred_probs_and_labels(sentence_probs[i], \n",
    "                                                   sentence_tokens[i], \n",
    "                                                   given_words[i], \n",
    "                                                   given_labels[i]) for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30126",
   "metadata": {},
   "source": [
    "## 6. Save `pred_probs` and `labels` \n",
    "\n",
    "Lastly, save the predictive probabilities and the given labels, which are flattened for easier storage. In addition, we save the number of word-level tokens in each sentence, which is a crucial information that we lose once they are flattened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3066803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = [pred_prob for pred_prob, _ in pred_probs_and_labels] \n",
    "labels = [label for _, label in pred_probs_and_labels] \n",
    "\n",
    "pred_probs_dict = to_dict(pred_probs) \n",
    "labels_dict = to_dict(labels) \n",
    "\n",
    "np.savez('pred_probs.npz', **pred_probs_dict) \n",
    "np.savez('labels.npz', **labels_dict) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
