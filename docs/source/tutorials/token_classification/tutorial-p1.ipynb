{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59db66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install transformers tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20de927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages version: \n",
    "# numpy == 1.23.0 \n",
    "# tqdm == 4.64.0 \n",
    "# transformers == 4.20.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Token Classification Label Error Detection - Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676421",
   "metadata": {},
   "source": [
    "In this tutorial, we show how you can retrieve the model-predictive probabilities and labels from a NLP token-classification dataset. These inputs are used to identify potential label issues in the dataset. \n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Use pre-trained HuggingFace models to get the predictive probabilities \n",
    "- Reduce subword-level tokens to word-level tokens\n",
    "\n",
    "\\* In most NLP literatures, tokens typically refer to words or punctuation marks, while most HuggingFace tokenizers break down longer words into subwords. To avoid confusion, given tokens are referred as \"word-level tokens\", and tokens obtained from the tokenizers as \"subword-level tokens\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0fb4a",
   "metadata": {},
   "source": [
    "Disable `TOKENIZERS_PARALLELISM` if multiple processors exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers tqdm \n",
    "import numpy as np\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from utils import * \n",
    "from tqdm import tqdm \n",
    "from bert import Ner \n",
    "from sklearn.metrics import balanced_accuracy_score \n",
    "\n",
    "import os \n",
    "if os.cpu_count() > 1: \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "    \n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e32604",
   "metadata": {},
   "source": [
    "## 2. Fetch the CONLL-2003 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced3d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# !wget https://data.deepai.org/conll2003.zip && mkdir data \n",
    "# !unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cd5a9",
   "metadata": {},
   "source": [
    "CONLL-2003 dataset is in the following format: \n",
    "\n",
    "`-DOCSTART- -X- -X- O` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of first sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "`[empty line]` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of second sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "In our example, we focus on the `ner_tags` (named-entity recognition tags), which include: \n",
    "\n",
    "| `ner_tags` |             Description              |\n",
    "|:----------:|:------------------------------------:|\n",
    "|     `O`    |      Other (not a named entity)      |\n",
    "|   `B-MIS`  | Beginning of a miscellaneous entity  |\n",
    "|   `I-MIS`  |         Miscellaneous entity         |\n",
    "|   `B-PER`  |     Beginning of a person entity     |\n",
    "|   `I-PER`  |            Person entity             |\n",
    "|   `B-ORG`  | Beginning of an organization entity  |\n",
    "|   `I-ORG`  |         Organization entity          |\n",
    "|   `B-LOC`  |    Beginning of a location entity    |\n",
    "|   `I-LOC`  |           Location entity            | \n",
    "\n",
    "For more information, see [here](https://paperswithcode.com/dataset/conll-2003). Here, all-caps words are casted into lowercase except for the first character (eg. `JAPAN` -> `Japan`). This is to discourage the tokenizer from breaking such words into multiple subwords. The `readfile` implementation is adapted from [here](https://github.com/kamalkraj/BERT-NER/blob/dev/run_ner.py#L92). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = [], [] \n",
    "\n",
    "for filepath in filepaths: \n",
    "    words, labels = readfile(filepath) \n",
    "    given_words.extend(words) \n",
    "    given_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148277",
   "metadata": {},
   "source": [
    "`given_words` and `given_labels` are in nested list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d505f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tLabel\tEntity\n",
      "-------------------------------\n",
      "Eu              5\tB-ORG     \n",
      "rejects         0\tO         \n",
      "German          1\tB-MISC    \n",
      "call            0\tO         \n",
      "to              0\tO         \n",
      "boycott         0\tO         \n",
      "British         1\tB-MISC    \n",
      "lamb            0\tO         \n",
      ".               0\tO         \n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "\n",
    "print('Word\\t\\tLabel\\tEntity\\n-------------------------------') \n",
    "for word, label in zip(given_words[i], given_labels[i]): \n",
    "    print('{:14s}{:3d}\\t{:10s}'.format(word, label, entities[label])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8639b",
   "metadata": {},
   "source": [
    "Next, obtain the sentences with some minor pre-processing for readability. Sentences that contain `#` or have length shorter than or equal to one character are removed. The first condition is to ensure that the symbol does not get confused with the same symbol representing subwords after the sentence is being tokenized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f19eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c496b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 20718\n",
      "Eu rejects German call to boycott British lamb.\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences: %d' % len(sentences)) \n",
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1b2b0",
   "metadata": {},
   "source": [
    "## 3. Train Models using Cross-Validation \n",
    "\n",
    "To identify potential label errors in the training dataset, we compute the out-of-sample predicted probabilities using cross-validation. We first partition the dataset into k-folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797df997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'folds/' already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "lines = [[]] \n",
    "for filepath in filepaths: \n",
    "    for line in open(filepath) : \n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(lines[-1]) > 0: \n",
    "                lines.append([]) \n",
    "        else: \n",
    "            lines[-1].append(line) \n",
    "        \n",
    "lines = lines[:-1] \n",
    "lines = [line for m, line in zip(mask, lines) if m] \n",
    "\n",
    "k = 5 \n",
    "indicies = create_folds(lines, k=k) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304653aa",
   "metadata": {},
   "source": [
    "We train one model for each training/testing pair: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c877f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 already exists, skipping...\n",
      "Model 1 already exists, skipping...\n",
      "Model 2 already exists, skipping...\n",
      "Model 3 already exists, skipping...\n",
      "Model 4 already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "for i in range(k): \n",
    "    if os.path.exists('folds/fold%d/model/' % i): \n",
    "        print('Model %d already exists, skipping...' % i) \n",
    "    else: \n",
    "        os.system(\n",
    "            \"python3 run_ner.py --data_dir=folds/fold%d --bert_model=bert-base-cased \" \\\n",
    "            \"--task_name=ner --output_dir=folds/fold%d/model --max_seq_length=256 \" \\\n",
    "            \"--do_train --num_train_epochs 10 --warmup_proportion=0.1\" % (i, i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621481",
   "metadata": {},
   "source": [
    "## 4. Compute Out-of-Sample Predicted Probabilities \n",
    "\n",
    "We obtain the predicted probabilities for each sample using the model in which the sample was held out from training. Note that most tokenizers break down sentences into subword-level tokens, which are units smaller than word-level tokens. For example, the following sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560f8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu rejects German call to boycott British lamb.\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "print(sentences[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d0119",
   "metadata": {},
   "source": [
    "is tokenized into: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08446655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '##u', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.']\n"
     ]
    }
   ],
   "source": [
    "model = Ner(\"folds/fold0/model/\") \n",
    "tokens = model.tokenize(sentences[i])[0] \n",
    "print(tokens) \n",
    "tokens = [token.replace('#', '') for token in tokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493203d3",
   "metadata": {},
   "source": [
    "`##` indicates that the token is a subword. We remove them and manually map them to the original tokens afterwards. We collect both the predictive probabilities and the tokens for each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fac29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [01:14<00:00, 55.86it/s]\n",
      "100%|██████████| 4144/4144 [01:14<00:00, 55.54it/s]\n",
      "100%|██████████| 4144/4144 [01:13<00:00, 56.07it/s]\n",
      "100%|██████████| 4143/4143 [01:14<00:00, 55.93it/s]\n",
      "100%|██████████| 4143/4143 [01:14<00:00, 55.74it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens, sentence_probs = {}, {} \n",
    "for i in range(k): \n",
    "    model = Ner(\"folds/fold%d/model/\" % i) \n",
    "    for index in tqdm(indicies[i]): \n",
    "        sentence_probs[index], sentence_tokens[index] = model.predict(sentences[index])  \n",
    "        \n",
    "sentence_tokens = [[token.replace('#', '') for token in sentence_tokens[i]] for i in range(len(sentences))] \n",
    "sentence_probs = [sentence_probs[i] for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edbe08",
   "metadata": {},
   "source": [
    "In this example, we are more interested in severe types of mislabels, such as `B-LOC` vs. `B-PER`, instead of `B-LOC` vs. `I-LOC`. Therefore, we discard the `B-` and `I-` prefixes, and get the model-predictive probabilities for each subword-level token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2cc02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "model_maps = [-1, 0, 1, 1, 2, 2, 3, 3, 4, 4, -1, -1] \n",
    "given_labels = [mapping(labels, maps=given_maps) for labels in given_labels] \n",
    "sentence_probs = [merge_probs(pred_prob, maps=model_maps) for pred_prob in sentence_probs] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3baf",
   "metadata": {},
   "source": [
    "## 5. Reducing from subword-level to word-level \n",
    "\n",
    "When a sentence gets tokenized, each word-level token may be broken down into subword-level tokens, each of which generates a predictive probability. Given that we only have the labels for word-level tokens, we reduce the subword-level tokens to word-level tokens. \n",
    "\n",
    "\\* For this example, most subwords-to-words reduction are handled internally, but for most other models the reduction has to be done manually. In the following, we show our method of reduction, which is slightly different from how the `bert` model reduces it. See [here](https://github.com/kamalkraj/BERT-NER/blob/dev/bert.py#L85) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43366dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tEu rejects German call to boycott British lamb.\n",
      "Given words:\t['Eu', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Subwords:\t['E', 'u', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', 'mb', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:\\t' + sentences[0]) \n",
    "print('Given words:\\t' + str(given_words[0])) \n",
    "print('Subwords:\\t' + str(tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eccd57",
   "metadata": {},
   "source": [
    "The word `lamb` is tokenized into two subwords `la` and `mb`. In this case, we assign the average predictive probabilities of the two subwords to the token. Alternatively, we can take the weighted average, such that the weight for each predictive probability is proportional to the length of its corresponding subword. This is to ensure that longer subwords have heavier weights on the average predictive probabilities, although the benefits are insignificant for most datasets. \n",
    "\n",
    "Each tokenizer tokenizes sentences differently. In some rare cases, a subword may overlap two tokens, resulting in a misalignment in tokenization. For example, consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0565e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tMassachusetts Institute of Technology (MIT)\n",
      "Given words:\t['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')']\n",
      "Subwords:\t['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')']\n"
     ]
    }
   ],
   "source": [
    "demo_sentence = 'Massachusetts Institute of Technology (MIT)' \n",
    "demo_given_words = ['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')'] \n",
    "demo_subwords = ['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')'] \n",
    "\n",
    "print('Sentence:\\t' + demo_sentence) \n",
    "print('Given words:\\t' + str(demo_given_words)) \n",
    "print('Subwords:\\t' + str(demo_subwords)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794f3d0",
   "metadata": {},
   "source": [
    "In this case, we assign the predictive probabilities of `(M` to `(`, and the average predictive probabilities of `(M` and `IT` to `MIT`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3656f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_and_labels = [get_pred_probs_and_labels(sentence_probs[i], \n",
    "                                                   sentence_tokens[i], \n",
    "                                                   given_words[i], \n",
    "                                                   given_labels[i]) for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30126",
   "metadata": {},
   "source": [
    "## 6. Save `pred_probs` and `labels` \n",
    "\n",
    "Lastly, save the predictive probabilities and the given labels, which are flattened for easier storage. In addition, we save the number of word-level tokens in each sentence, which is a crucial information that we lose once they are flattened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3066803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = [pred_prob for pred_prob, _ in pred_probs_and_labels] \n",
    "labels = [label for _, label in pred_probs_and_labels] \n",
    "\n",
    "pred_probs_dict = to_dict(pred_probs) \n",
    "labels_dict = to_dict(labels) \n",
    "\n",
    "np.savez('pred_probs.npz', **pred_probs_dict) \n",
    "np.savez('labels.npz', **labels_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2d51a",
   "metadata": {},
   "source": [
    "## 7. Model evaluation  \n",
    "\n",
    "Lastly, we evaluate the model accuracy. We use the definition of precision and recall introduced by CoNLL-2003: \n",
    "\n",
    "> *“precision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file.”*\n",
    "\n",
    "See [here](https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d285325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t\t0.911\n",
      "Recall\t\t\t0.916\n",
      "f1-score\t\t0.913\n",
      "Accuracy\t\t0.976\n",
      "Balanced Accuracy\t0.923\n"
     ]
    }
   ],
   "source": [
    "predictions = [pred_prob.argmax(axis=1) for pred_prob in pred_probs] \n",
    "predictions_flatten = [pred for prediction in predictions for pred in prediction] \n",
    "given_labels_flatten = [label for given_label in given_labels for label in given_label] \n",
    "\n",
    "counts = [0, 0, 0, 0] \n",
    "correct = 0 \n",
    "\n",
    "for truth, prediction in zip(given_labels_flatten, predictions_flatten): \n",
    "    if truth != 0: \n",
    "        if truth == prediction: \n",
    "            counts[0] += 1 \n",
    "        counts[1] += 1 \n",
    "    if prediction != 0: \n",
    "        if truth == prediction: \n",
    "            counts[2] += 1 \n",
    "        counts[3] += 1 \n",
    "    if truth == prediction: \n",
    "        correct += 1 \n",
    "        \n",
    "precision = counts[2] / counts[3] \n",
    "recall = counts[0] / counts[1] \n",
    "f1 = 2 * precision * recall / (precision + recall) \n",
    "accuracy = correct / len(given_labels_flatten) \n",
    "\n",
    "balanced_accuracy = balanced_accuracy_score(given_labels_flatten, predictions_flatten) \n",
    "\n",
    "print('Precision\\t\\t%.3f\\nRecall\\t\\t\\t%.3f\\nf1-score\\t\\t%.3f\\nAccuracy\\t\\t%.3f\\nBalanced Accuracy\\t%.3f' % \n",
    "     (precision, recall, f1, accuracy, balanced_accuracy)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
