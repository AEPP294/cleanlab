{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Token Classification Label Error Detection - Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676421",
   "metadata": {},
   "source": [
    "In this tutorial, we show how you can retrieve the model-predictive probabilities and labels from a NLP token-classification dataset. These inputs are used to identify potential label issues in the dataset. \n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Use pre-trained HuggingFace models to get the predictive probabilities \n",
    "- Reduce subword-level tokens to word-level tokens\n",
    "\n",
    "\\* In most NLP literatures, tokens typically refer to words or punctuation marks, while most HuggingFace tokenizers break down longer words into subwords. To avoid confusion, given tokens are referred as \"word-level tokens\", and tokens obtained from the tokenizers as \"subword-level tokens\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0fb4a",
   "metadata": {},
   "source": [
    "Disable `TOKENIZERS_PARALLELISM` if multiple processors exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1349304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ericw/.local/lib/python3.9/site-packages (4.20.1)\n",
      "Requirement already satisfied: tqdm in /home/ericw/.local/lib/python3.9/site-packages (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ericw/.local/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ericw/.local/lib/python3.9/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ericw/.local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.18.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ericw/.local/lib/python3.9/site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ericw/.local/lib/python3.9/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ericw/.local/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ericw/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ericw/.local/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.9/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.9'\n",
      "Call stack:\n",
      "  File \"/home/ericw/.local/bin/pip\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 952, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ericw/.local/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tqdm \n",
    "import numpy as np\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tqdm import tqdm \n",
    "\n",
    "import os \n",
    "if os.cpu_count() > 1: \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cd5a9",
   "metadata": {},
   "source": [
    "## 2. Fetch the CONLL-2003 dataset \n",
    "\n",
    "CONLL-2003 dataset is in the following format: \n",
    "\n",
    "`-DOCSTART- -X- -X- O` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of first sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "`[empty line]` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of second sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "In our example, we focus on the `ner_tags` (named-entity recognition tags), which include: \n",
    "\n",
    "| `ner_tags` |             Description              |\n",
    "|:----------:|:------------------------------------:|\n",
    "|     `O`    |      Other (not a named entity)      |\n",
    "|   `B-MIS`  | Beginning of a miscellaneous entity  |\n",
    "|   `I-MIS`  |         Miscellaneous entity         |\n",
    "|   `B-PER`  |     Beginning of a person entity     |\n",
    "|   `I-PER`  |            Person entity             |\n",
    "|   `B-ORG`  | Beginning of an organization entity  |\n",
    "|   `I-ORG`  |         Organization entity          |\n",
    "|   `B-LOC`  |    Beginning of a location entity    |\n",
    "|   `I-LOC`  |           Location entity            | \n",
    "\n",
    "For more information, see [here](https://paperswithcode.com/dataset/conll-2003). Here, all-caps words are casted into lowercase except for the first character (eg. `JAPAN` -> `Japan`). This is to discourage the tokenizer from breaking such words into multiple subwords. The `readfile` implementation is adapted from [here](https://github.com/kamalkraj/BERT-NER/blob/dev/run_ner.py#L92). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data/conll.txt'\n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = readfile(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148277",
   "metadata": {},
   "source": [
    "`given_words` and `given_labels` are in nested list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d505f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tLabel\tEntity\n",
      "-------------------------------\n",
      "Soccer          0\tO         \n",
      "-               0\tO         \n",
      "Japan           7\tB-LOC     \n",
      "Get             0\tO         \n",
      "Lucky           0\tO         \n",
      "Win             0\tO         \n",
      ",               0\tO         \n",
      "China           3\tB-PER     \n",
      "In              0\tO         \n",
      "Surprise        0\tO         \n",
      "Defeat          0\tO         \n",
      ".               0\tO         \n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "\n",
    "print('Word\\t\\tLabel\\tEntity\\n-------------------------------') \n",
    "for word, label in zip(given_words[i], given_labels[i]): \n",
    "    print('{:14s}{:3d}\\t{:10s}'.format(word, label, entities[label])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8639b",
   "metadata": {},
   "source": [
    "Next, obtain the sentences with some minor pre-processing for readability. Sentences that contain `#` or have length shorter than or equal to one character are removed. The first condition is to ensure that the symbol does not get confused with the same symbol representing subwords after the sentence is being tokenized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f19eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(words): \n",
    "    sentence = ''\n",
    "    for word in words:\n",
    "        if word not in string.punctuation or word in ['-', '(']:\n",
    "            word = ' ' + word\n",
    "        sentence += word\n",
    "    sentence = sentence.replace(\" '\", \"'\").replace('( ', '(').strip()\n",
    "    return sentence\n",
    "\n",
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "mask = [len(sentence) > 1 and '#' not in sentence for sentence in sentences] \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "sentences = [sentence for m, sentence in zip(mask, sentences) if m] \n",
    "\n",
    "n = len(sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c496b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3449\n",
      "Soccer - Japan Get Lucky Win, China In Surprise Defeat.\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences: %d' % n) \n",
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d30f9",
   "metadata": {},
   "source": [
    "## 3. Pre-trained model \n",
    "\n",
    "In this example, we use `dslim/bert-base-NER` (`bert`) as our NLP model and tokenizer. Note that most tokenizers break down sentences into subword-level tokens, which are units smaller than word-level tokens. Since we are only interested in label issues at a word level, we first need to know how each sentence is tokenized. Feel free to try out an alternative model - `xlm-roberta-large-fintuned-conll03-english` (`xlm`)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374f0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dslim/bert-base-NER' \n",
    "# model_name = 'xlm-roberta-large-finetuned-conll03-english'\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline(task=\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4c26e",
   "metadata": {},
   "source": [
    "For example, the following sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87c67b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soccer - Japan Get Lucky Win, China In Surprise Defeat.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4a534",
   "metadata": {},
   "source": [
    "is tokenized into: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e17ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Sur', '##prise', 'De', '##fe', '##at', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer(sentences[0])['input_ids'] \n",
    "tokens = [tokenizer.decode(token) for token in token_ids] \n",
    "print(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d7a62",
   "metadata": {},
   "source": [
    "`[CLS]` and `[SEP]` are special tokens to help the BERT model to identify the start and end of each sentence, and `##` indicates that the token is a subword. We remove them and manually map them to the original tokens afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4a82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Sur', 'prise', 'De', 'fe', 'at', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.replace('#', '') for token in tokens][1:-1] \n",
    "print(tokens) \n",
    "sentence_tokens = [[tokenizer.decode(token) for token in tokenizer(sentence)['input_ids']] for sentence in sentences] \n",
    "sentence_tokens = [[token.replace('#', '') for token in sentence_token][1:-1] for sentence_token in sentence_tokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e1fc2",
   "metadata": {},
   "source": [
    "## 4. Get predictive probabilities \n",
    "\n",
    "In this example, we are more interested in severe types of mislabels, such as `B-LOC` vs. `B-PER`, instead of `B-LOC` vs. `I-LOC`. Therefore, we discard the `B-` and `I-` prefixes, and get the model-predictive probabilities for each subword-level token. Set `merge_entities=False` if you do not wish to merge*. \n",
    "\n",
    "\\* Due to a slightly different set of named entities in the `xlm` model, we HAVE to discard the prefixes. For more information, see [here](https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/blob/main/config.json). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e199b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 3449/3449 [00:59<00:00, 57.55it/s]\n"
     ]
    }
   ],
   "source": [
    "merge_entities = True \n",
    "merge_entities = merge_entities or model_name == 'xlm-roberta-large-finetuned-conll03-english' \n",
    "\n",
    "if merge_entities: \n",
    "    merge = lambda idx: (idx+1) // 2 \n",
    "    merge_list = lambda l: list(map(merge, l)) \n",
    "    given_labels = [merge_list(labels) for labels in given_labels] \n",
    "\n",
    "def get_probs(sentence, merge_method='dslim/bert-base-NER'): \n",
    "    def softmax(logit): \n",
    "        return np.exp(logit) / np.sum(np.exp(logit)) \n",
    "    \n",
    "    forward = pipe.forward(pipe.preprocess(sentence)) \n",
    "    logits = forward['logits'][0].numpy() \n",
    "    probs = np.array([softmax(logit) for logit in logits]) \n",
    "    probs = probs[1:-1] \n",
    "    \n",
    "    if not merge_entities: \n",
    "        return probs \n",
    "    \n",
    "    if merge_method == 'dslim/bert-base-NER': \n",
    "        probs_merged = np.zeros([len(probs), 5]) \n",
    "        for i in range(9): \n",
    "            merged_idx = merge(i) \n",
    "            probs_merged[:, merged_idx] += probs[:, i] \n",
    "        return probs_merged \n",
    "    \n",
    "    elif merge_method == 'xlm-roberta-large-finetuned-conll03-english': \n",
    "        def xlm_mapping(prob): \n",
    "            new_prob = np.zeros(5) \n",
    "\n",
    "            new_prob[0] = prob[7] \n",
    "            new_prob[1] = prob[1] + prob[4] \n",
    "            new_prob[2] = prob[6] \n",
    "            new_prob[3] = prob[2] + prob[5] \n",
    "            new_prob[4] = prob[0] + prob[3] \n",
    "\n",
    "            return new_prob \n",
    "\n",
    "        return np.array(list(map(xlm_mapping, probs))) \n",
    "\n",
    "sentence_probs = [get_probs(sentence, merge_method=model_name) for sentence in tqdm(sentences)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3baf",
   "metadata": {},
   "source": [
    "## 5. Reducing from subword-level to word-level \n",
    "\n",
    "Reduce subword-level tokens to word-level tokens. Here, we show an example of how the reduction is implemented. Consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43366dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tSoccer - Japan Get Lucky Win, China In Surprise Defeat.\n",
      "Given words:\t['Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Surprise', 'Defeat', '.']\n",
      "Subwords:\t['Soccer', '-', 'Japan', 'Get', 'Lucky', 'Win', ',', 'China', 'In', 'Sur', 'prise', 'De', 'fe', 'at', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:\\t' + sentences[0]) \n",
    "print('Given words:\\t' + str(given_words[0])) \n",
    "print('Subwords:\\t' + str(sentence_tokens[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eccd57",
   "metadata": {},
   "source": [
    "The token `Surprise` is tokenized into two subwords `Sur` and `prise`. In this case, we assign the average predictive probabilities of the two subwords to the token. Alternatively, we can take the weighted average, such that the weight for each predictive probability is proportional to the length of its corresponding subword. This is to ensure that longer subwords have heavier weights on the average predictive probabilities, although the benefits are insignificant for most datasets. \n",
    "\n",
    "Each tokenizer tokenizes sentences differently. In some rare cases, a subword may overlap two tokens, resulting in a misalignment in tokenization. For example, consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0565e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tMassachusetts Institute of Technology (MIT)\n",
      "Given words:\t['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')']\n",
      "Subwords:\t['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')']\n"
     ]
    }
   ],
   "source": [
    "demo_sentence = 'Massachusetts Institute of Technology (MIT)' \n",
    "demo_given_words = ['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')'] \n",
    "demo_subwords = ['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')'] \n",
    "\n",
    "print('Sentence:\\t' + demo_sentence) \n",
    "print('Given words:\\t' + str(demo_given_words)) \n",
    "print('Subwords:\\t' + str(demo_subwords)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794f3d0",
   "metadata": {},
   "source": [
    "In this case, we assign the predictive probabilities of `(M` to `(`, and the average predictive probabilities of `(M` and `IT` to `MIT`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3656f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_probs_and_labels(scores, tokens, given_token, given_label, weighted=False): \n",
    "    i, j = 0, 0 \n",
    "    pred_probs, labels = [], [] \n",
    "    for token, label in zip(given_token, given_label): \n",
    "        i_new, j_new = i, j \n",
    "        acc = 0 \n",
    "        \n",
    "        weights = []         \n",
    "        while acc != len(token): \n",
    "            token_len = len(tokens[i_new][j_new:]) \n",
    "            remain = len(token) - acc \n",
    "            weights.append(min(remain, token_len)) \n",
    "            if token_len > remain: \n",
    "                acc += remain \n",
    "                j_new += remain \n",
    "            else: \n",
    "                acc += token_len \n",
    "                i_new += 1 \n",
    "                j_new = 0 \n",
    "        \n",
    "        if i != i_new: \n",
    "            probs = np.average(scores[i:i_new], axis=0, weights=weights if weighted else None) \n",
    "        else: \n",
    "            probs = scores[i] \n",
    "        i, j = i_new, j_new \n",
    "        \n",
    "        pred_probs.append(probs) \n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(pred_probs), labels \n",
    "\n",
    "pred_probs_and_labels = [get_pred_probs_and_labels(sentence_probs[i], \n",
    "                                                   sentence_tokens[i], \n",
    "                                                   given_words[i], \n",
    "                                                   given_labels[i]) for i in range(n)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30126",
   "metadata": {},
   "source": [
    "## 6. Save `pred_probs` and `labels` \n",
    "\n",
    "Lastly, save the predictive probabilities and the given labels, which are flattened for easier storage. In addition, we save the number of word-level tokens in each sentence, which is a crucial information that we lose once they are flattened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3066803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = [pred_prob for pred_prob, _ in pred_probs_and_labels] \n",
    "labels = [label for _, label in pred_probs_and_labels] \n",
    "\n",
    "pred_probs = np.array([prob for pred_prob in pred_probs for prob in pred_prob]) \n",
    "labels = np.array([l for label in labels for l in label]) \n",
    "\n",
    "sentence_lengths = np.array([len(tokens) for tokens in given_words]) \n",
    "\n",
    "np.save('pred_probs.npy', pred_probs) \n",
    "np.save('labels.npy', labels) \n",
    "np.save('sentence_lengths.npy', sentence_lengths) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
