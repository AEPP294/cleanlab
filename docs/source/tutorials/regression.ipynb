{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Quality Scores for Regression with Noisy Labels "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quickstart tutorial shows how to use cleanlab for finding label quality scores in regression datasets. Using the approach mentioned here, you can find label quality scores in any regression dataset irrespective of modality i.e. tabular, text, image, etc. \n",
    "\n",
    "**This example will take you through the following:**\n",
    "- Generate label quality scores for each example in the dataset. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickstart \n",
    "\n",
    "Cleanlab uses two inputs to generate scores for labels in the dataset:\n",
    "- `labels`: NumPy array of given labels in the dataset. labels[i] should contain label for `i`-th example. \n",
    "- `predictions`: NumPy array of predictions generated through your favorite regressor. predictions[i] should contain predicted value for `i`-th example. \n",
    "\n",
    "If you already have predictions from your regressor, you can generate label quality scores for each example using the code below: \n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">\n",
    "\n",
    "```python \n",
    "\n",
    "from cleanlab.regression.rank import get_label_quality_scores\n",
    "label_quality_scores = get_label_quality_scores(labels, predictions)\n",
    "\n",
    "```\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies and import them \n",
    "You can use `pip` to install all the packages required for this tutorial as follows:\n",
    "\n",
    "`!pip install cleanlab xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# Package versions we used: xgboost==1.7.2\n",
    "\n",
    "dependencies = [\"cleanlab\", \"xgboost\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = \" \".join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from cleanlab.regression.rank import get_label_quality_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# This cell is hidden on docs.cleanlab.ai\n",
    "np.set_printoptions(suppress=True)\n",
    "SEED = np.random.RandomState(10203)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we have added a support function to plot the dataset for a quick demonstration. You can use it to highlight the examples based on label_quality_scores. You can skip this part and move to the next section. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>See the code for visualization **(click to expand)** </summary>\n",
    "\n",
    "```python \n",
    "# Note: this pulldown is for docs.cleanlab.ai, if running on local Jupyter or colab, please ignore it. \n",
    "\n",
    "def plot_data(\n",
    "    data_x, data_y, circles, title, alpha=0.6, color=\"#1f77b4\", colorbar=False, xlabel=\"\", ylabel=\"\"\n",
    "):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    data_x = data_x.to_numpy()\n",
    "    data_y = data_y.to_numpy()\n",
    "    plt.scatter(data_x, data_y, c=color, s=30)\n",
    "    for i in circles:\n",
    "        plt.plot(\n",
    "            data_x[i],\n",
    "            data_y[i],\n",
    "            \"o\",\n",
    "            markerfacecolor=\"none\",\n",
    "            markeredgecolor=\"red\",\n",
    "            markersize=10,\n",
    "            markeredgewidth=2.5,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar(orientation=\"vertical\")\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(\n",
    "    data_x, data_y, circles, title, alpha=0.6, color=\"#1f77b4\", colorbar=False, xlabel=\"\", ylabel=\"\"\n",
    "):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    data_x = data_x.to_numpy()\n",
    "    data_y = data_y.to_numpy()\n",
    "    plt.scatter(data_x, data_y, c=color, s=30)\n",
    "    for i in circles:\n",
    "        plt.plot(\n",
    "            data_x[i],\n",
    "            data_y[i],\n",
    "            \"o\",\n",
    "            markerfacecolor=\"none\",\n",
    "            markeredgecolor=\"red\",\n",
    "            markersize=10,\n",
    "            markeredgewidth=2.5,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar(orientation=\"vertical\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import dataset and Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the data\n",
    "!wget -nc https://cleanlab-public.s3.amazonaws.com/Datasets/student_grades.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"./student_grades.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data frame displayed above, `grade` represents the noisy grades and `true_grade` represents the ground truth. Please note that ground truth is usually not available in a real dataset. We have added it here for comparison and to demonstrate our method. Also, note that column `notes` have categorical information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate true errors\n",
    "true_errors = (data[\"grade\"] != data[\"true_grade\"]).astype(int)\n",
    "true_errors_index = np.where(true_errors == 1)[0]\n",
    "plot_data(\n",
    "    data_x=data[\"exam_3\"],\n",
    "    data_y=data[\"grade\"],\n",
    "    circles=true_errors_index,\n",
    "    title=\"Noisy regression dataset\",\n",
    "    xlabel=\"exam_3 feature\",\n",
    "    ylabel=\"grade (Y value)\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, `grade (Y value)` is plotted against one of the features in the dataset (`exam_3`). We have circled the examples that were considered as `true_error` in **Red**. \n",
    "\n",
    "Let's check some of the errors in next cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the error in the dataset\n",
    "display(\"Errors in dataset:\", data.loc[true_errors_index].head())\n",
    "\n",
    "# Dropping ground truth i.e. true_grade \n",
    "data = data.drop(columns=[\"true_grade\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sklearn API `XGBRegressor` from `xgboost` as the regressor for this tutorial. `xgboost` provides easy to use interface to process categorical variables. In order to make inputs compatible with `xgboost`, we need to divide data in `X` and labels `y`. This is demonstrated in the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost automatically factors categorical variable, you just need to mark the columns as category\n",
    "data.notes = data.notes.astype(\"category\")\n",
    "\n",
    "# xgboost takes data and label seperately, so you will need to divide data accordingly.\n",
    "X = data.drop(columns=[\"grade\"])\n",
    "y = data[\"grade\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with initializing the model with relevant parameters. As mentioned earlier we are using `xgboost` for this tutorial. To handle categorical variables, we specifically need to set `enable_categorical` flag to `True`. Note that, support for the categorical variable is in the experimental stage and doesn't support the auto-selection of `tree_method`. Therefore, you will need to specify `tree_method` from supported types. More details can be found [here](https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_crossval_folds = 5\n",
    "\n",
    "model = XGBRegressor(\n",
    "    tree_method= \"hist\",\n",
    "    n_estimators = 10,  \n",
    "    enable_categorical = True, \n",
    "    random_state = SEED)\n",
    "\n",
    "# get predictions using cross-validation\n",
    "predictions = cross_val_predict(\n",
    "    estimator=model, X=X, y=y, cv=num_crossval_folds, method = \"predict\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional benefit of cross-validation is that it facilitates more reliable evaluation of our model than a single training/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = r2_score(y, predictions)\n",
    "print(f\"R-squared on predictions from cross-validation: {roc:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using cleanlab to generate label quality scores\n",
    "\n",
    "Once you have the predictions from the cross-validation. You can generate label quality scores using cleanlab by running just one line of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label quality score for each example in the dataset using cleanlab\n",
    "label_quality_scores = get_label_quality_scores(labels=y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(\n",
    "    data_x=data[\"exam_3\"],\n",
    "    data_y=data[\"grade\"],\n",
    "    circles=true_errors_index,\n",
    "    color=label_quality_scores,\n",
    "    title=\"Noisy regression dataset colored by label quality scores\",\n",
    "    colorbar=True,\n",
    "    xlabel=\"exam_3 feature\",\n",
    "    ylabel=\"grade (Y value)\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we have colored each examples with their label quality scores generated using cleanlab. Examples are same as earlier plot displayed in the notebook.  `grade (Y value)` is plotted against one of the features in the dataset (`exam_3`)\n",
    "\n",
    "**Red circle** represents the errors in `grade` with respect to the ground truth `true_grade`.  You can observe that our method assign low scores to examples that were considered as `true_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "label_quality_scores = get_label_quality_scores(labels=y, predictions=predictions)\n",
    "label_quality_scores_residual = get_label_quality_scores(labels = y, predictions=predictions, method=\"residual\")\n",
    "\n",
    "auc = roc_auc_score(true_errors, 1 - label_quality_scores)\n",
    "\n",
    "if auc <= 0.5:\n",
    "    raise ValueError(\"Label quality scores did not perform well enough\")\n",
    "\n",
    "if auc <= roc_auc_score(true_errors, 1 - label_quality_scores_residual):\n",
    "    raise ValueError(\"Label quality scores did not outperform alternative scores\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ed33b5e6ac3d9870092cd802185bba6fb7a8302b6022e7097221f18c33cb7b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
