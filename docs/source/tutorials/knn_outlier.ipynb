{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f65acd",
   "metadata": {},
   "source": [
    "# Detecting Outliers with Cleanlab's outlier detection algorithm\n",
    "\n",
    "This 5-minute quickstart tutorial shows how to detect potential outliers in image classification data. The dataset used is `cifar10` which contains 60,000 images. Each 32x32 pixels and belonging to 1 of 10 categories.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Load the [`cifar10`](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and do some basic data pre-processing.\n",
    "- Create a `trainX` set with less categories than `testX`\n",
    "- analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e987fa",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies\n",
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install torch\n",
    "!pip install cleanlab\n",
    "...\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install git+https://github.com/cleanlab/cleanlab.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b552a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# If running on Colab, may want to use GPU (select: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "\n",
    "dependencies = [\"cleanlab\", \"matplotlib\", \"torch\", \"pylab\", \"keras\", \"sklearn\", \"timm\", ]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e76f30",
   "metadata": {},
   "source": [
    "Lets first set some seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4de93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "warnings.filterwarnings(\"ignore\", \"Lazy modules are a new feature.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d0618",
   "metadata": {},
   "source": [
    "## 2. Fetch and scale the Cifar10 dataset\n",
    "\n",
    "After some basic preprocessing, we manually remove some categories from the training data, making them outliers in the test set. For this example we've chosen to remove all categories that are not an animal (airplane, automobile, ship, truck) from the training set `trainX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "from keras.datasets import cifar10, cifar100\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae35d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar10 datasets\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "trainX, _, trainy, _ = train_test_split(trainX, trainy, test_size=0.5, stratify=trainy, random_state=SEED) # only use 50% of each label\n",
    "\n",
    "# Convert from integers to floats and normalize range 0-1\n",
    "trainX = trainX.astype('float32') / 255.0\n",
    "testX = testX.astype('float32') / 255.0\n",
    "\n",
    "# Manually remove non-animals out of the training dataset\n",
    "animal_labels = [2,3,4,5,6,7]\n",
    "animal_idxs = np.where(np.isin(trainy, animal_labels))[0] # find idx of animals\n",
    "trainX = trainX[animal_idxs]\n",
    "trainy = trainy[animal_idxs]\n",
    "\n",
    "# Check the shapes of our training and test sets\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4480c85",
   "metadata": {},
   "source": [
    "#### Lets visualize some of the training and test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_labels = {0: 'airplane', \n",
    "              1: 'automobile', \n",
    "              2: 'bird',\n",
    "              3: 'cat', \n",
    "              4: 'deer', \n",
    "              5: 'dog', \n",
    "              6: 'frog', \n",
    "              7: 'horse', \n",
    "              8:'ship', \n",
    "              9:'truck'}\n",
    "\n",
    "def plot_images(X,y):\n",
    "    plt.rcParams[\"figure.figsize\"] = (9,7)\n",
    "    for i in range(15):\n",
    "        # define subplot\n",
    "        ax = plt.subplot(3,5,i+1)\n",
    "        ax.set_title(txt_labels[int(y[i])])\n",
    "        # plot raw pixel data\n",
    "        ax.imshow(X[i])\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486acf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(trainX,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce43e4c",
   "metadata": {},
   "source": [
    "Observe how there are only animals left in the training set `trainX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(testX,testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf11eb8",
   "metadata": {},
   "source": [
    "The test set on the other hand still visibily contains the non-animal images: ship, airplane, automobile, truck. If we consider `trainX` to be the representation of data, then these non-animal images in `testX` become outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f57bae",
   "metadata": {},
   "source": [
    "## 3. Import a model and embeddings\n",
    "The model we are importing comes from `timm` (`timm` is a deep-learning library created by Ross Wightman and is a collection of SOTA models and utilities).\n",
    "\n",
    "The model itself is a `resnet50`. This can be done with any model capable of generating feature embeddings (which we can then use as representations to define the indistribution content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd151d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=0) # download the model from timm\n",
    "trainX_torch = torch.from_numpy(trainX.swapaxes(1,3)) # turn trainX into tensor and fix channel dimension\n",
    "train_feature_embeddings = model(trainX_torch) # fit model to train images and get train embeddings\n",
    "train_feature_embeddings = train_feature_embeddings.detach().numpy() # change type to numpy array\n",
    "print(f'Pooled shape: {train_feature_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=0) # download the model from timm\n",
    "testX_torch = torch.from_numpy(testX.swapaxes(1,3)) # turn trainX into tensor and fix channel dimension\n",
    "test_feature_embeddings = model(testX_torch) # fit model to train images and get train embeddings\n",
    "test_feature_embeddings = test_feature_embeddings.detach().numpy() # change type to numpy array\n",
    "print(f'Pooled shape: {test_feature_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83792c57",
   "metadata": {},
   "source": [
    "## 4. Import cleanlab and find outliers in the dataset\n",
    "We can use the `cleanlab` library to try and find the artificially added outlier examples (airplanes, automobiles, boats) in the test data. We can also check the training data to find any naturally occuring outlier examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from cleanlab.rank import get_outlier_scores\n",
    "from sklearn.neighbors import NearestNeighbors # import KNN estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49026d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KNN estimator and fit it on the resnet50 feature embeddings\n",
    "knn = NearestNeighbors(n_neighbors=10).fit(train_feature_embeddings)\n",
    "\n",
    "# get outlier scores for the test feature embeddings\n",
    "outlier_scores = get_outlier_scores(features=test_feature_embeddings, knn=knn, k=10)\n",
    "\n",
    "# visualize top 15 outlier scores\n",
    "top_outlier_idxs = (outlier_scores).argsort()[:15]\n",
    "plot_images(testX[top_outlier_idxs],testy[top_outlier_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25a681",
   "metadata": {},
   "source": [
    "Notice how majority of the outliers belong to the holdout classes (airplane, automobile, ship). These feature representations are futher away in the model representation space than the training dataset representations. \n",
    "\n",
    "Just for fun, lets visualize what the NearestNeighbors algorithm considers the 15 least probable outliers in our test set. Notice there are a lot less images from the houldout classes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize top 15 outlier scores\n",
    "top_outlier_idxs = (outlier_scores).argsort()[:15]\n",
    "plot_images(testX[top_outlier_idxs],testy[top_outlier_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf08aa3",
   "metadata": {},
   "source": [
    "We can also compute the precision/recall of our algorithm for the examples. [Jonas is this important?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "animal_labels = [2,3,4,5,6,7] # identify animal labels in the testing dataset\n",
    "animal_idxs = np.where(np.isin(testy, animal_labels))[0] # find idx of animals\n",
    "not_outlier = np.zeros(len(testy), dtype=bool) # is outlier\n",
    "not_outlier[animal_idxs] = True\n",
    "precision, recall, thresholds = precision_recall_curve(not_outlier, 1 - outlier_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\", fontsize=14)\n",
    "plt.ylabel(\"Precision\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692f978",
   "metadata": {},
   "source": [
    "### Finding naturally occuring outlier examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76571ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outlier scores for our train feature embeddings\n",
    "outlier_scores = get_outlier_scores(features=train_feature_embeddings, k=10)\n",
    "\n",
    "# visualize top 15 outlier scores\n",
    "top_outlier_idxs = (-outlier_scores).argsort()[:15]\n",
    "plot_images(trainX[top_outlier_idxs],trainy[top_outlier_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d203a",
   "metadata": {},
   "source": [
    "Just for fun, lets see what our model considers the least likeley outliers in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize bottom 12 outlier scores on train set\n",
    "top_outlier_idxs = (outlier_scores).argsort()[:15]\n",
    "plot_images(trainX[top_outlier_idxs],trainy[top_outlier_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cleanlab can discover two kinds of labels for you! \"what if our training data contained SOME outliers (little)\"\n",
    "# add a couple outliers to the training (map these labels to training stuff + flip cifar10 labels? same %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Add test cell\n",
    "# # Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "# highlighted_indices = [59915, 24798, 59701, 50340]  # verify these examples were found by find_label_issues\n",
    "# if not all(x in ranked_label_issues for x in highlighted_indices):\n",
    "#     raise Exception(\"Some highlighted examples are missing from ranked_label_issues.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanlab-p38",
   "language": "python",
   "name": "cleanlab-p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
