{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f65acd",
   "metadata": {},
   "source": [
    "# Detecting Outliers with Cleanlab and PyTorch Image Models (timm)\n",
    "\n",
    "This 5-minute quickstart tutorial shows how to detect potential outliers in image classification data using Cleanlab and PyTorch. The dataset used is `cifar10` which contains 60,000 images. Each image belongs to 1 of 10 categories: `[airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]`. \n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Load the [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and do some basic data pre-processing.\n",
    "- Create `trainset` and `testset` such that `trainset` only contains animals and `testset` contains all categories.\n",
    "- Load pretrained `timm` model and extract `trainset` and `testset` feature embeddings.\n",
    "- Use `cleanlab` to find naturally occuring outlier examples in the `trainset`.\n",
    "- Use `cleanlab` to find outlier examples (non-animals) in the `testset`.\n",
    "- Explore threshold selection for labeling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e987fa",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies\n",
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install torch\n",
    "!pip install cleanlab\n",
    "...\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install git+https://github.com/cleanlab/cleanlab.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b552a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# If running on Colab, may want to use GPU (select: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "# Package versions we used: matplotlib==3.5.1, numpy==1.21.6, torch==1.11.0, scikit-learn==1.0.2, torchvision==0.12.0, timm==0.5.4, cleanlab==2.0.0\n",
    "\n",
    "dependencies = ['builtins','torch','torchvision','torchvision.transforms','numpy','matplotlib.pyplot','warnings','cleanlab','timm']\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e76f30",
   "metadata": {},
   "source": [
    "Lets first import the required packages and set some seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4de93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import warnings\n",
    "\n",
    "import cleanlab\n",
    "from cleanlab.rank import get_outlier_scores\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.neighbors import NearestNeighbors # import KNN estimator\n",
    "import timm # resnet50 pre-trained model\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "warnings.filterwarnings(\"ignore\", \"Lazy modules are a new feature.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d0618",
   "metadata": {},
   "source": [
    "## 2. Fetch and scale the Cifar10 dataset\n",
    "\n",
    "Import `cifar10` dataset. After some basic preprocessing, we manually remove some categories from the training examples thereby making them outliers in the test set. For this example we've chosen to remove all categories that are not an animal `[airplane, automobile, ship, truck]` from the training set `trainX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783460c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select how to load the cifar10 datasets. Load into tensors for training.\n",
    "transform_normalize = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# Load cifar10 datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_normalize)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_normalize)\n",
    "\n",
    "# Manually remove non-animals out of the training dataset\n",
    "animal_labels = [2,3,4,5,6,7]\n",
    "trainy = trainset.targets # Get labels\n",
    "animal_idxs = np.where(np.isin(trainy, animal_labels))[0]\n",
    "trainset  = torch.utils.data.Subset(trainset, animal_idxs) # Select subset of animals for our trainset\n",
    "\n",
    "# Check the shapes of our training and test sets\n",
    "print('Trainset length: %s' % (len(trainset)))\n",
    "print('Testset length: %s' % (len(testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4480c85",
   "metadata": {},
   "source": [
    "#### Lets visualize some of the training and test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_labels = {0: 'airplane', \n",
    "              1: 'automobile', \n",
    "              2: 'bird',\n",
    "              3: 'cat', \n",
    "              4: 'deer', \n",
    "              5: 'dog', \n",
    "              6: 'frog', \n",
    "              7: 'horse', \n",
    "              8:'ship', \n",
    "              9:'truck'}\n",
    "\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    return np.transpose(npimg, (1, 2, 0))\n",
    "\n",
    "def plot_images(dataset):\n",
    "    plt.rcParams[\"figure.figsize\"] = (9,7)\n",
    "    for i in range(15):\n",
    "        X,y = dataset[i]\n",
    "        ax = plt.subplot(3,5,i+1)\n",
    "        ax.set_title(txt_labels[int(y)])\n",
    "        ax.imshow(imshow(X))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddf313",
   "metadata": {},
   "source": [
    "Observe how there are only animals left in the training set `trainset` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486acf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf11eb8",
   "metadata": {},
   "source": [
    "The test set on the other hand still visibily contains the non-animal images: `[ship, airplane, automobile, truck]`. If we consider `trainset` to be the representative of the normal data distribution then these non-animal images in `testset` become outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f57bae",
   "metadata": {},
   "source": [
    "## 3. Import a model and get embeddings\n",
    "We pass in the images into the model to generate embeddings in the feature space that we require as inputs for the outlier detection algorithm. \n",
    "\n",
    "The model we are importing is a `resnet50` that comes from [timm](https://timm.fast.ai/), a deep-learning library collection of SOTA models and utilities but outlier detection can be done with any method capable of generating feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ddeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from timm and set to eval mode\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37785b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for more efficient data streaming to the model\n",
    "batch_size = 50\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature embeddings of the training data using the model\n",
    "# This cell can take several mins\n",
    "train_feature_embeddings = []\n",
    "\n",
    "for data in trainloader:\n",
    "    images, labels = data\n",
    "    with torch.no_grad():\n",
    "        feature_embeddings = model(images)\n",
    "        train_feature_embeddings.extend(feature_embeddings.numpy())\n",
    "train_feature_embeddings = np.array(train_feature_embeddings)\n",
    "\n",
    "print(f'Train embeddings pooled shape: {train_feature_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe10df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature embeddings of the test data using the model\n",
    "# This cell can take several mins\n",
    "test_feature_embeddings = []\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    with torch.no_grad():\n",
    "        feature_embeddings = model(images)\n",
    "        test_feature_embeddings.extend(feature_embeddings.numpy())\n",
    "test_feature_embeddings = np.array(test_feature_embeddings)\n",
    "print(f'Test embeddings pooled shape: {test_feature_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83792c57",
   "metadata": {},
   "source": [
    "## 4. Use cleanlab to find outliers in the dataset\n",
    "With just the feature embeddings and ``cleanlab`` we can now identify outliers in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692f978",
   "metadata": {},
   "source": [
    "### Finding naturally occuring outlier examples in the trainset\n",
    "\n",
    "Calling ``get_outlier_score()`` on ``train_feature_embeddings`` will find any naturally occuring outliers in ``trainset``. These examples should be animal images that look strange or different from the majority of other animal images in the data. It is not often you come across a cat wearing a turquoise beanie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76571ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get outlier scores for our train feature embeddings\n",
    "train_outlier_scores = get_outlier_scores(features=train_feature_embeddings)\n",
    "\n",
    "# Visualize top 15 most likeley outlier scores\n",
    "top_train_outlier_idxs = (train_outlier_scores).argsort()[:15]\n",
    "top_train_outlier_subset = torch.utils.data.Subset(trainset, top_train_outlier_idxs)\n",
    "plot_images(top_train_outlier_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d203a",
   "metadata": {},
   "source": [
    "Just for fun, lets see what our model considers the least likeley outliers in the training set! These examples look very homogeneous as they are very close together in the feature space to many of their neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 15 least probable outlier scores on trainset\n",
    "bottom_train_outlier_idxs = (-train_outlier_scores).argsort()[:15]\n",
    "bottom_train_outlier_subset = torch.utils.data.Subset(trainset, bottom_train_outlier_idxs)\n",
    "plot_images(bottom_train_outlier_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c680a",
   "metadata": {},
   "source": [
    "### Finding outlier examples in the testset\n",
    "We can also use ``get_outlier_score()`` to find the artificially added outlier classes `[airplanes, automobiles, trucks, boats]` into the test dataset.\n",
    "\n",
    "We will begin with creating an ``sklearn.neighbors.NearestNeighbor`` estimator to fit on the ``train_feature_embeddings`` (i.e. the train dataset). The function can be called without passing in a ``knn`` object in which case one will be created internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49026d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN estimator and fit it on the train feature embeddings\n",
    "knn = NearestNeighbors(n_neighbors=20,metric=\"cosine\").fit(train_feature_embeddings)\n",
    "\n",
    "# Get outlier scores for the test feature embeddings\n",
    "test_outlier_scores = get_outlier_scores(features=test_feature_embeddings, knn=knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25ed8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Not sure which distance metric to use?\n",
    "    \n",
    "\n",
    "By default `sklearn.neighbors.NearestNeighbor` uses` minkowski` distance, but we generally recommend using `cosine` distance instead when computing distances between neural net representations of data. Internally, `get_outlier_score()` uses sklearn's KNN based on `cosine` distance already.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 15 most likeley outlier scores\n",
    "top_outlier_idxs = (test_outlier_scores).argsort()[:15]\n",
    "top_outlier_subset = torch.utils.data.Subset(testset, top_outlier_idxs)\n",
    "plot_images(top_outlier_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25a681",
   "metadata": {},
   "source": [
    "Notice how all of shown outliers identified in `testset` belong to the holdout classes `[airplane, automobile, ship, truck]`. These feature representations are futher away in the feature space than the feature representations of animal images also found in `testset`.\n",
    "\n",
    "Just for fun, lets visualize what the `NearestNeighbors` algorithm considers the 15 least probable outliers in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 15 least likeley to be outlier scores\n",
    "bottom_outlier_idxs = (-test_outlier_scores).argsort()[:15]\n",
    "bottom_outlier_subset = torch.utils.data.Subset(testset, bottom_outlier_idxs)\n",
    "plot_images(bottom_outlier_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf08aa3",
   "metadata": {},
   "source": [
    "Notice there are a lot less images from the out of distribution classes here and all the images are visually similar to each other. Even the shon ``automobile`` and ``airplane`` examples look similar to their animal conterparts.\n",
    "\n",
    "We can also compute the precision/recall curve of our algorithm for the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_labels = [2,3,4,5,6,7] # Animal labels to identify in the dataset\n",
    "animal_idxs = np.where(np.isin(testset.targets, animal_labels))[0] # Find animal idxs\n",
    "not_outlier = np.zeros(len(testset.targets), dtype=bool)\n",
    "not_outlier[animal_idxs] = True\n",
    "precision, recall, thresholds = precision_recall_curve(not_outlier, 1 - test_outlier_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\", fontsize=14)\n",
    "plt.ylabel(\"Precision\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc49a9",
   "metadata": {},
   "source": [
    "## 4. Thresholding outliers in outlier_scores\n",
    "\n",
    "Now that we know how to find the outlier scores, how do we determine how many of the lowest ranked indices in ``testset`` should be marked as outliers? We can use the `train_outlier_scores` distribution to calcualte a threshold for the `testset`.\n",
    "\n",
    "If we want to select a hard threshold for outlier detection on the future test data that gives us around 5% false positives. We can look at the distribution of outlier_scores for the `trainset` (assuming it has no outliers) and use the 5-th percentile of this distribution as the threshold below which to call a test example an outlier.\n",
    "\n",
    "Lets first take a look at our score distributions and see where the 5th precentile falls (along red line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 5th percentile of the trainset distribution\n",
    "fifth_percentile = np.percentile(train_outlier_scores, 5)\n",
    "\n",
    "# Plot outlier_score distributions and the 5th percentile cutoff\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "plt_range = [min(train_outlier_scores.min(),test_outlier_scores.min()), \\\n",
    "             max(train_outlier_scores.max(),test_outlier_scores.max())]\n",
    "axes[0].hist(train_outlier_scores, range=plt_range, bins=50)\n",
    "axes[0].set(title='train_outlier_scores distribution', ylabel='Frequency')\n",
    "axes[0].axvline(x=fifth_percentile, color='red', linewidth=2)\n",
    "axes[1].hist(test_outlier_scores, range=plt_range, bins=50)\n",
    "axes[1].set(title='test_outlier_scores distribution', ylabel='Frequency')\n",
    "axes[1].axvline(x=fifth_percentile, color='red', linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308466c6",
   "metadata": {},
   "source": [
    "Everything to the left of the red line in the distribution of `test_outlier_scores` will be marked as an outlier. Let's measure how well this works. \n",
    "\n",
    "**Visually check the threshold**\n",
    "\n",
    "First lets plot the least sure outliers of our `testset`. These are the images immediately to the left of that cuttoff line. As you can see majority of them are still true outliers however there are a few less standard looking animals that are now falseley identified as outliers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88631fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 15 outlier scores right along the cuttoff (i.e. the least likeley outliers given threshold)\n",
    "outlier_idxs = test_outlier_scores.argsort()\n",
    "outlier_scores = test_outlier_scores[outlier_idxs]\n",
    "selected_outlier_idxs = outlier_idxs[outlier_scores < fifth_percentile]\n",
    "\n",
    "selected_outlier_subset = torch.utils.data.Subset(testset, selected_outlier_idxs[::-1])\n",
    "plot_images(selected_outlier_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c5247",
   "metadata": {},
   "source": [
    "**Empirically measure threshold effectiveness**\n",
    "\n",
    "Setting the hard threshold to the 5-th percentile of the `trainset` gives us almost exactly a 5% false positive rate. If that is what we are looking for, this is an effective threshold cuttoff for this data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ca669",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_labels = [2,3,4,5,6,7] # Animal labels in the dataset\n",
    "animal_idxs = np.where(np.isin(testset.targets, animal_labels))[0]\n",
    "\n",
    "false_positive_idxs = set(selected_outlier_idxs).intersection(set(animal_idxs))\n",
    "FPR = len(false_positive_idxs) / (len(false_positive_idxs) + len(animal_idxs))\n",
    "print(f'Number of false positives detected: {len(false_positive_idxs)}\\nFalse positive rate: {round(FPR,4)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "top_outlier_idxs_test = [1579, 4373, 9898, 3485, 2701, 3768, 7033, 8413, 7217, 8311, 5814, 3789, 3740, 2059, 5056]\n",
    "top_outlier_idxs_train = [26999, 20198,7967,29061,16684,2558,23072,454,5815,9967,27499,22507,8488,20685,9880]\n",
    "\n",
    "if not all(x in top_outlier_idxs for x in top_outlier_idxs_test):\n",
    "    raise Exception(\"Some highlighted examples are missing from top outliers in test set.\")\n",
    "\n",
    "if not all(x in top_train_outlier_idxs for x in top_outlier_idxs_train):\n",
    "    raise Exception(\"Some highlighted examples are missing from bottom test set outliers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanlab-p38",
   "language": "python",
   "name": "cleanlab-p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
