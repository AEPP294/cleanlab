{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Datalab` class is a new feature of cleanlab that helps you identify issues in your machine learning datasets that may negatively impact the performance of your machine learning model if not addressed.\n",
    "\n",
    "This includes:\n",
    "\n",
    "- Finding noisy labels\n",
    "- Detecting outliers\n",
    "- Spotting near duplicates\n",
    "\n",
    "\n",
    "In this tutorial, we will walk throug the process of using `Datalab` to identify issues in a (toy) dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installation\n",
    "\n",
    "\n",
    "\n",
    "`Datalab` has additional dependencies that are not included in the standard installation of cleanlab.\n",
    "\n",
    "To install everything necessary for this tutorial, run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn matplotlib\n",
    "# !pip install git+https://github.com/cleanlab/cleanlab.git#egg=cleanlab[datalab]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then import the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scripts.data_generation import create_data, plot_data\n",
    "from cleanlab import Datalab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We'll load a toy dataset for this tutorial.\n",
    "The dataset has two numerical features and a label column with three classes.\n",
    "\n",
    "The true label for each example is determined by the sum of the features and is assigned to one of three bins.\n",
    "\n",
    "\n",
    "A dataset is created and stored in a dictionary called `data_dict`. The following variables are extracted for convenience:\n",
    "\n",
    "- `X_train`: A matrix of features for the training set.\n",
    "- `noisy_labels`: A vector of noisy labels for *the training set* (represented as strings).\n",
    "- `y_train_idx`: A vector of true labels for the training set (represented as integers).\n",
    "- `noisy_labels_idx`: Noisy labels for the training set (represented as integers).\n",
    "- `X_out`: A matrix of features for the examples that are manually added as outliers.\n",
    "  - This also contains a pair of near duplicate examples.\n",
    "  - The examples are also in `X_train`.\n",
    "\n",
    "Below we also print out the label accuracy for the training set (the proportion of examples whose noisy label matches the true label), then plot the features of the training with each set of labels.\n",
    "\n",
    "Noisy labels are highlighted in red if they do not match the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = create_data()\n",
    "\n",
    "X_train, noisy_labels, y_train_idx, noisy_labels_idx, X_out = (\n",
    "    data_dict[key]\n",
    "    for key in [\n",
    "        \"X_train\", \"noisy_labels\", \"y_train_idx\", \"noisy_labels_idx\", \"X_out\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, y_train_idx, noisy_labels_idx, X_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, in real-world scenarios, you may not have information about the true labels or the distribution of the features a priori.\n",
    "\n",
    "There may be exceptions, such as when you have extensive domain knowledge or have carefully verified each data point by hand. However, this is not a guarantee that the data is completely error-free. Some nuances of the data may not be immediately apparent and can be difficult to capture during the data collection process.\n",
    "\n",
    "Hence, we'll assume that we are unaware of the true labels and the typical distribution of the features.\n",
    "\n",
    "`Datalab` has several ways of loading the data.\n",
    "In this case we'll wrap the training features and noisy labels in a dictionary so that we can pass it to `Datalab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"X\": X_train, \"y\": noisy_labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get out-of-sample predicted probabilities from a classifier\n",
    "\n",
    "To perform certain issue checks, `Datalab` relies on out-of-sample predicted probabilities from a trained model.\n",
    "\n",
    "For the purposes of this tutorial, we will use a simple logistic regression model \n",
    "and the cross_val_predict() function from scikit-learn to generate out-of-sample predicted probabilities for the training set.\n",
    "\n",
    "This allows us to demonstrate how to use `Datalab` to detect label errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "pred_probs = cross_val_predict(\n",
    "    estimator=model, X=data[\"X\"], y=data[\"y\"], cv=5, method=\"predict_proba\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a Datalab object\n",
    "\n",
    "We'll instantiate a `Datalab` and provide it with the data object and a name of the label column in the data object.\n",
    "\n",
    "We'll use the `find_issues` method of `Datalab` to identify issues in the dataset.\n",
    "This method accepts out-of-sample predicted probabilities and the feature data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = Datalab(data, label_name=\"y\")\n",
    "lab.find_issues(pred_probs=pred_probs, features=data[\"X\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review the results of the issue checks using the `report` method of `Datalab`.\n",
    "This method provides a comprehensive summary of each type of issue found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on a particular issue, there are several methods that can be used to access the results of the issue checks.\n",
    "\n",
    "For example, we fetch summarized statistics on a particular issue using the `get_summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.get_summary(\"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see results of an issue check for all examples using the `get_issues` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.get_issues(\"label\").head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some intermediate results from a particular issue check can be accessed using the `get_info` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.get_info(\"label\")\n",
    "\n",
    "for k, v in lab.get_info(\"label\").items():\n",
    "    str_v = str(v)\n",
    "    n = 50\n",
    "    if len(str_v) > n:\n",
    "        str_v = str_v[:n] + \"...\"\n",
    "    print(f\"{k}: {str_v}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are also directly accessible as attributes of the `Datalab` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\", lab.issue_summary, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "print(\"Issues:\", lab.issues.head(), sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "# print(\"Info:\", lab.info)  # Too long to print"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental issue search\n",
    "\n",
    "We can call `find_issues` multiple times on a `Datalab` object to find issues incrementally.\n",
    "\n",
    "This is done via the `issue_types` argument which accepts a dictionary of issue types and any corresponding keyword arguments that may be updated.\n",
    "\n",
    "Notice how the call to `find_issues` updates the output of the call to `report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = Datalab(data, label_name=\"y\")\n",
    "lab.find_issues(pred_probs=pred_probs, issue_types={\"outlier\": {}})\n",
    "lab.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the next check\n",
    "lab.find_issues(pred_probs=pred_probs, issue_types={\"label\": {}})\n",
    "lab.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous checks can be overwritten\n",
    "lab.find_issues(pred_probs=pred_probs, features=data[\"X\"], issue_types={\"outlier\": {}})\n",
    "lab.report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing hyperparameter in issue search\n",
    "\n",
    "In this example, we'll use custom hyperparameters for the `\"outlier\"` and `\"near_duplicate\"` issue types.\n",
    "\n",
    "We can also increase the verbosity of the `report` by increasing the `verbosity` argument, to show additional information about the issue checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = Datalab(data, label_name=\"y\")\n",
    "knn = NearestNeighbors(n_neighbors=3, metric=\"euclidean\")\n",
    "issue_types = {\n",
    "    \"outlier\": {\"ood_kwargs\": {\"params\": {\"knn\": knn}}},\n",
    "    \"near_duplicate\": {\"metric\": \"euclidean\"},\n",
    "}\n",
    "\n",
    "lab.find_issues(\n",
    "    pred_probs=pred_probs,\n",
    "    features=data[\"X\"],\n",
    "    issue_types=issue_types,\n",
    ")\n",
    "lab.report(k=10, verbosity=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the number of issues has changed after adjusting the hyperparameters for the check for outliers and near duplicates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a custom IssueManager\n",
    "\n",
    "\n",
    "All issue checks in `Datalab` are implemented as subclasses of an `IssueManager` class provided by cleanlab.\n",
    "\n",
    "You may come up with your own issue checks that are not included in cleanlab.\n",
    "\n",
    "To integrate these into `Datalab`, you must create a subclass of `IssueManager` and add it to to a registry of issue managers that is used by `Datalab`.\n",
    "\n",
    "Here, we'll create an arbitrary issue check that checks the divisibility of an example's index in the dataset by 13.\n",
    "\n",
    "The necessary members to implement in the subclass are:\n",
    "\n",
    "- A class variable called `issue_name` that acts as a unique identifier for the type of issue.\n",
    "  - This is further used to derive other necessary attributes of the issue manager.\n",
    "- A method called `find_issues` that:\n",
    "  - Gives a quality score for each example in the dataset, in terms of how unlikely it is to be an issue.\n",
    "  - Tag each example as an issue or not.\n",
    "  - Combine these in a dataframe that is assigned to an `issues` attribute of the issue manager.\n",
    "  - Define a summarized score for the entire dataset, to set a `summary` attribute of the issue manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.experimental.datalab.issue_manager import IssueManager\n",
    "from cleanlab.experimental.datalab.factory import register\n",
    "\n",
    "\n",
    "def scoring_function(idx: int) -> float:\n",
    "    if idx == 0:\n",
    "        # Zero excluded from the divisibility check, gets the highest score\n",
    "        return 1\n",
    "    rem = idx % 13\n",
    "    inv_scale = idx // 13\n",
    "    if rem == 0:\n",
    "        return 0.5 * (1 - np.exp(-0.1*(inv_scale-1)))\n",
    "    else:\n",
    "        return 1 - 0.49 * (1 - np.exp(-inv_scale**0.5))*rem/13\n",
    "\n",
    "\n",
    "@register\n",
    "class SuperstitionIssueManager(IssueManager):\n",
    "    \"\"\"A custom issue manager that keeps track of issue indices that\n",
    "    are divisible by 13.\n",
    "    \"\"\"\n",
    "    description: str = \"Examples with indices that are divisible by 13 may be unlucky.\"  # Optional\n",
    "    issue_name: str = \"superstition\"\n",
    "\n",
    "    def find_issues(self, div=13, **_) -> None:\n",
    "        ids = self.datalab.issues.index.to_series()\n",
    "        issues_mask = ids.apply(lambda idx: idx % div == 0 and idx != 0)\n",
    "        scores = ids.apply(scoring_function)\n",
    "        self.issues = pd.DataFrame(\n",
    "            {\n",
    "                f\"is_{self.issue_name}_issue\": issues_mask,\n",
    "                self.issue_score_key: scores,\n",
    "            },\n",
    "        )\n",
    "        summary_score = 1 - sum(issues_mask) / len(issues_mask)\n",
    "        self.summary = self.get_summary(score = summary_score)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once registered, this issue manager performs custom issue checks when `find_issues` is called on a `Datalab` instance.\n",
    "\n",
    "As the `Datalab` instance already has results from the outlier and near duplicate checks, the custom issue check is performed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.find_issues(issue_types={\"superstition\": {}})\n",
    "lab.report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load Datalab objects\n",
    "\n",
    "We can save the Datalab object to a file using the `save` method.\n",
    "\n",
    "A directory path needs to be provided as an argument to the `save` method.\n",
    "\n",
    "The same directory path can be used to load the Datalab object using the `load` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test-lab\"\n",
    "lab.save(path)\n",
    "\n",
    "new_lab = Datalab.load(path)\n",
    "new_lab.report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
