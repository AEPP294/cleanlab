{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe0d62e",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "\n",
    "Answers to frequently asked questions about the [cleanlab](https://github.com/cleanlab/cleanlab) open source package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c277a86",
   "metadata": {},
   "source": [
    "### What data can cleanlab detect issues in?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4713c2c",
   "metadata": {},
   "source": [
    "Any classification data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766a0ac",
   "metadata": {},
   "source": [
    "### Why isnâ€™t CleanLearning working for my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfec97",
   "metadata": {},
   "source": [
    "At this time, cleanlab only works for numpy matrices or pd.DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca36874",
   "metadata": {},
   "source": [
    "### How do I format labels for Cleanlab?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0fbb3",
   "metadata": {},
   "source": [
    "Cleanlab only works with integer-encoded labels in the range `{0,1, ... K-1}` where `K = number_of_classes`. The `labels` array should only contain integer values in the range  `{0, K-1}` and be of shape `(N,)` where `N = total_number_of_data_points`.\n",
    "\n",
    "**Text or string labels** should to be mapped to integers for eaach possible value. For example if your original data labels look like this: `[\"dog\", \"dog\", \"cat\", \"mouse\", \"cat\"]`, you should feed them to Cleanlab like this: `labels = [1,1,0,2,0]` and keep track of which integer uniquely represents which class (classes were ordered alphabetically in this example). \n",
    "\n",
    "**One-hot encoded labels** should be integer-encoded by finding the argmax along the one-hot encoded axis. An example of what this might look like is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239d5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# This example arr has 4 labels (one per data point) where \n",
    "# each label can be one of 3 possible classes\n",
    "\n",
    "arr  = np.array([[0,1,0],[1,0,0],[0,0,1],[1,0,0]])\n",
    "labels = np.argmax(arr, axis=1) # How labels should be represented when passed into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0e3e3",
   "metadata": {},
   "source": [
    "### How can I use different models for cleaning and final training in CleanLearning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0e729",
   "metadata": {},
   "source": [
    "Here's how to use one type of model for finding label issues and another type of model for the final training on the clean subset of data with label issues removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e83b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "yp\n",
      "Cannot utilize sample weights for final training. To utilize must either specify noise_matrix or have previously called self.find_label_issues() instead of filter.find_label_issues()\n",
      "GradientBoostingClassifier()\n"
     ]
    }
   ],
   "source": [
    "from cleanlab.classification import CleanLearning\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Make data\n",
    "data = np.vstack([np.random.random((100, 2)), np.random.random((100, 2)) + 10])\n",
    "y = np.array([0] * 100 + [1] * 100)\n",
    "# Introduce label errors\n",
    "true_errors = [97, 98, 100, 101, 102, 104]\n",
    "for idx in true_errors:\n",
    "\ty[idx] = 1 - y[idx]  # Flip label\n",
    "\n",
    "# Demonstrate CleanLearning with 2 different classifiers:\n",
    "\n",
    "model_to_find_errors = LogisticRegression()  # this model will be trained many times  via cross-validation\n",
    "model_to_return = GradientBoostingClassifier()  # this model will be trained once on clean subset of data\n",
    "cl0 = CleanLearning(model_to_find_errors)\n",
    "issues = cl0.find_label_issues(data, y)\n",
    "print(cl0.clf)  # will be LogisticRegression()\n",
    "\n",
    "cl = CleanLearning(model_to_return).fit(data, y, label_issues=issues)\n",
    "pred_probs = cl.predict_proba(data)  # predictions from GradientBoostingClassifier\n",
    "print(cl.clf)  # will be GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913aac7",
   "metadata": {},
   "source": [
    "### How do I hyperparameter tune only the final model (and not the one used by CleanLearning)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec803163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cleanlab.classification import CleanLearning\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Make data\n",
    "data = np.vstack([np.random.random((100, 2)), np.random.random((100, 2)) + 10])\n",
    "y = np.array([0] * 100 + [1] * 100)\n",
    "\n",
    "# Introduce label errors\n",
    "true_errors = [97, 98, 100, 101, 102, 104]\n",
    "for idx in true_errors:\n",
    "    y[idx] = 1 - y[idx]  # Flip label\n",
    "\n",
    "# Demonstrate CleanLearning with no hyperparameter-tuning to find label issues\n",
    "# but hyperparameter-tuning for the final training of model on clean subset of the data:\n",
    "model_to_find_errors = GradientBoostingClassifier()  # this model will be trained many times  via cross-validation\n",
    "model_to_return = RandomizedSearchCV(GradientBoostingClassifier(),\n",
    "                    param_distributions = {\n",
    "                        \"learning_rate\": [0.001, 0.05, 0.1, 0.2, 0.5],\n",
    "                        \"max_depth\": [3, 5, 10],\n",
    "                    }\n",
    "                )   # this model will be trained once on clean subset of data\n",
    "cl0 = CleanLearning(model_to_find_errors)\n",
    "issues = cl0.find_label_issues(data, y)\n",
    "\n",
    "cl = CleanLearning(model_to_return).fit(data, y, label_issues=issues)\n",
    "pred_probs = cl.predict_proba(data)  # predictions from hyperparameter-tuned GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520a93f",
   "metadata": {},
   "source": [
    "### Can't find an answer to your question?\n",
    "\n",
    "If your question is not addressed in these tutorials, please refer to the: [Cleanlab Github issues](https://github.com/cleanlab/cleanlab/issues?q=is%3Aissue), [Cleanlab Code Examples](https://github.com/cleanlab/examples) or our [Slack Community](https://join.slack.com/t/cleanlab-community/shared_invite/zt-17lszn4hv-gg2FhZPXYfljq_l01uo92g).\n",
    "\n",
    "If your question is not addressed anywhere, please open a [new Github issue](https://github.com/cleanlab/cleanlab/issues/new/choose). Our developers can also provide personalized assistance in [Slack](https://join.slack.com/t/cleanlab-community/shared_invite/zt-17lszn4hv-gg2FhZPXYfljq_l01uo92g)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
