{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7436b8",
   "metadata": {},
   "source": [
    "# Find Best Consensus Labels for Multiannotator Data using Cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b432513",
   "metadata": {},
   "source": [
    "In this tutorial, we will use Cleanlab to find improved consensus labels for data that has been labeled by multiple annotators. The tutorial also shows you how `cleanlab.multiannotator.get_label_quality_multiannotator()` can automatically compute:\n",
    "- consensus quality scores\n",
    "- annotator quality scores\n",
    "- agreement scores for each example and each annotator\n",
    "- label quality scores for every annotator's labels\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "- Obtain consensus labels of multiannotator data using majority vote\n",
    "- Train a model on the majority vote consensus labels to compute out-of-sample predicted probabilites\n",
    "- Use cleanlab's `multiannotator.get_label_quality_multiannotator` function to get improved consensus labels\n",
    "- View other information about your multiannotator dataset, such as consensus and annotator quality scores, agreement scores, detailed label quality scores and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a48d31",
   "metadata": {},
   "source": [
    "## 1. Install and import required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e5b15",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install sklearn\n",
    "!pip install cleanlab\n",
    "\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install git+https://github.com/cleanlab/cleanlab.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "dependencies = [\"cleanlab\", \"sklearn\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0148e6",
   "metadata": {},
   "source": [
    "Letâ€™s import some of the packages needed throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator, get_majority_vote_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0592b43",
   "metadata": {},
   "source": [
    "## 2. Create the data (can skip these details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d16faa",
   "metadata": {},
   "source": [
    "<details><summary>Below is the code used for data-generation.</summary>\n",
    "\n",
    "```ipython3\n",
    "# Note: This pulldown content is for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "    \n",
    "from cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace\n",
    "from cleanlab.benchmarking.noise_generation import generate_noisy_labels\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "def make_data(\n",
    "    means=[[3, 2], [7, 7], [0, 8]],\n",
    "    covs=[[[5, -1.5], [-1.5, 1]], [[1, 0.5], [0.5, 4]], [[5, 1], [1, 5]]],\n",
    "    sizes=[80, 40, 40],\n",
    "    avg_trace=0.8,\n",
    "    num_annotators=50,\n",
    "    seed=SEED,  # set to None for non-reproducible randomness\n",
    "):\n",
    "    np.random.seed(seed=SEED)\n",
    "\n",
    "    m = len(means)  # number of classes\n",
    "    n = sum(sizes)\n",
    "    local_data = []\n",
    "    labels = []\n",
    "\n",
    "    for idx in range(m):\n",
    "        local_data.append(\n",
    "            np.random.multivariate_normal(mean=means[idx], cov=covs[idx], size=sizes[idx])\n",
    "        )\n",
    "        labels.append(np.array([idx for i in range(sizes[idx])]))\n",
    "    X_train = np.vstack(local_data)\n",
    "    true_labels_train = np.hstack(labels)\n",
    "\n",
    "    # Compute p(true_label=k)\n",
    "    py = np.bincount(true_labels_train) / float(len(true_labels_train))\n",
    "\n",
    "    noise_matrix = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=avg_trace * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Generate our noisy labels using the noise_matrix for specified number of annotators.\n",
    "    s = pd.DataFrame(\n",
    "        np.vstack(\n",
    "            [generate_noisy_labels(true_labels_train, noise_matrix) for _ in range(num_annotators)]\n",
    "        ).transpose()\n",
    "    )\n",
    "\n",
    "    # Each annotator only labels approximately 20% of the dataset\n",
    "    # (unlabeled points represented with NaN)\n",
    "    s = s.apply(lambda x: x.mask(np.random.random(n) < 0.8))\n",
    "    s.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    row_NA_check = pd.notna(s).any(axis=1)\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train[row_NA_check],\n",
    "        \"true_labels_train\": true_labels_train[row_NA_check],\n",
    "        \"multiannotator_labels\": s[row_NA_check].reset_index(drop=True),\n",
    "    }\n",
    "\n",
    "data_dict = make_data()\n",
    "data = data_dict[\"multiannotator_labels\"]  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a7d32",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "from cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace\n",
    "from cleanlab.benchmarking.noise_generation import generate_noisy_labels\n",
    "\n",
    "SEED = 111\n",
    "\n",
    "def make_data(\n",
    "    means=[[3, 2], [7, 7], [0, 8]],\n",
    "    covs=[[[5, -1.5], [-1.5, 1]], [[1, 0.5], [0.5, 4]], [[5, 1], [1, 5]]],\n",
    "    sizes=[80, 40, 40],\n",
    "    num_annotators=50,\n",
    "    seed=SEED,  # set to None for non-reproducible randomness\n",
    "):\n",
    "    np.random.seed(seed=SEED)\n",
    "\n",
    "    m = len(means)  # number of classes\n",
    "    n = sum(sizes)\n",
    "    local_data = []\n",
    "    labels = []\n",
    "\n",
    "    for idx in range(m):\n",
    "        local_data.append(\n",
    "            np.random.multivariate_normal(mean=means[idx], cov=covs[idx], size=sizes[idx])\n",
    "        )\n",
    "        labels.append(np.array([idx for i in range(sizes[idx])]))\n",
    "    X_train = np.vstack(local_data)\n",
    "    true_labels_train = np.hstack(labels)\n",
    "\n",
    "    # Compute p(true_label=k)\n",
    "    py = np.bincount(true_labels_train) / float(len(true_labels_train))\n",
    "    \n",
    "    noise_matrix_better = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=0.9 * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    noise_matrix_worse = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=0.5 * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Generate our noisy labels using the noise_matrix for specified number of annotators.\n",
    "    s = pd.DataFrame(\n",
    "        np.vstack(\n",
    "            [\n",
    "                generate_noisy_labels(true_labels_train, noise_matrix_better)\n",
    "                if i < num_annotators - 5\n",
    "                else generate_noisy_labels(true_labels_train, noise_matrix_worse)\n",
    "                for i in range(num_annotators)\n",
    "            ]\n",
    "        ).transpose()\n",
    "    )\n",
    "\n",
    "    # Each annotator only labels approximately 10% of the dataset\n",
    "    # (unlabeled points represented with NaN)\n",
    "    s = s.apply(lambda x: x.mask(np.random.random(n) < 0.9))\n",
    "    s.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    row_NA_check = pd.notna(s).any(axis=1)\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train[row_NA_check],\n",
    "        \"true_labels_train\": true_labels_train[row_NA_check],\n",
    "        \"multiannotator_labels\": s[row_NA_check].reset_index(drop=True),\n",
    "    }\n",
    "\n",
    "data_dict = make_data()\n",
    "\n",
    "X = data_dict[\"X_train\"]\n",
    "multiannotator_labels = data_dict[\"multiannotator_labels\"]\n",
    "true_labels = data_dict[\"true_labels_train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50cfbd",
   "metadata": {},
   "source": [
    "For this tutorial we are using a toy dataset that has 50 annotators and 160 examples. There are three possible classes, `0`, `1` and `2`.\n",
    "\n",
    "Each annotator annotates approximately 10% of the examples. We also synthetically made the last 5 annotators in our toy dataset have much noisier labels than the rest of the annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756fc69",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bringing Your Own Data (BYOD)?\n",
    "\n",
    "You can easily replace the above with your own multiannotator dataset, and continue with the rest of the tutorial.\n",
    " \n",
    "`multiannotator_labels` should be a numpy array or pandas DataFrame with each column representing an annotator and each row representing an example. Your classes (and entries of `multiannotator_labels`) should be represented as integer indices 0, 1, ..., num_classes - 1, where examples that are not annotated by a particular annotator are represented using `np.nan`.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639afa6",
   "metadata": {},
   "source": [
    "## 3. Get majority vote label and compute out-of-sample predicted probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197615f",
   "metadata": {},
   "source": [
    "Before training our machine learning model, we must first obtain the consensus labels from the annotators that labeled the data. The simplest way to obtain an initial set of consensus labels is to select it using majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e64dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_label = get_majority_vote_label(multiannotator_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19217d7b",
   "metadata": {},
   "source": [
    "Next, we will train our model on the consensus labels obtained using majority vote to compute out-of-sample predicted probabilities. Here, we use a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "num_crossval_folds = 5  \n",
    "cv_pred_probs = cross_val_predict(\n",
    "    estimator=model, X=X, y=majority_vote_label, cv=num_crossval_folds, method=\"predict_proba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8fc8a",
   "metadata": {},
   "source": [
    "## 4. Use cleanlab to get better consensus labels and other statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb5931",
   "metadata": {},
   "source": [
    "Using the annotators' labels and the out-of-sample predicted probabilites from the model, cleanlab can help us obtain improved consensus labels for our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiannotator_dict = get_label_quality_multiannotator(multiannotator_labels, cv_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f544a7",
   "metadata": {},
   "source": [
    "Here, we use the `multiannotator.get_label_quality_multiannotator()` function which returns a dictionary containing three items:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45811c",
   "metadata": {},
   "source": [
    "- `label_quality_multiannotator` which gives us the improved consensus labels using information from each of the annotators and the model. The DataFrame also contains information about the number of annotations, annotator agreement and consensus quality score for each example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f5509",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multiannotator_dict[\"label_quality_multiannotator\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63140e",
   "metadata": {},
   "source": [
    "- `detailed_label_quality` which returns the label quality score for each label given by every annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e905f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiannotator_dict[\"detailed_label_quality\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007edd87",
   "metadata": {},
   "source": [
    "- `annotator_stats` which gives us the annotator quality score for each annotator, alongisde other information such as the number of examples each annotator labeled, their agreement with the consensus label and the class they perform the worst at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bccbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiannotator_dict[\"annotator_stats\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22cbcc7",
   "metadata": {},
   "source": [
    "The `annotator_stats` DataFrame is sorted by increasing `annotator_quality`, showing us the worst annotators first.\n",
    "\n",
    "Notice that in the above table annotators with ids 45 to 49 have the worst annotator quality score, which is expected because we made the last 5 annotators systematically worse than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f0875",
   "metadata": {},
   "source": [
    "### Comparing improved consensus labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1eb5bc",
   "metadata": {},
   "source": [
    "We can get the improved consensus labels from the `label_quality_multiannotator` DataFrame shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca558bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_consensus_label = multiannotator_dict[\"label_quality_multiannotator\"][\"consensus_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258de7e5",
   "metadata": {},
   "source": [
    "Since our toy dataset is synthetically generated by adding noise to each annotator's labels, we know the ground truth labels for each example. Hence we can compare the accuracy of the consensus labels obtained using majority vote, and the improved consensus labels obtained using cleanlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2e144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "majority_vote_accuracy = np.mean(true_labels == majority_vote_label)\n",
    "cleanlab_label_accuracy = np.mean(true_labels == improved_consensus_label)\n",
    "\n",
    "print(f\"Accuracy of majority vote labels = {majority_vote_accuracy}\")\n",
    "print(f\"Accuracy of cleanlab consensus labels = {cleanlab_label_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03413d2",
   "metadata": {},
   "source": [
    "We can see that the accuracy of the consensus labels improved as a result of using cleanlab, which not only takes the annotators' labels into account, but also a model to compute better consensus labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1617664",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "if majority_vote_accuracy >= cleanlab_label_accuracy:  # check cleanlab has improved prediction accuracy\n",
    "    raise Exception(\"Cleanlab training failed to improve model accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "50292dbb1f747f7151d445135d392af3138fb3c65386d17d9510cb605222b10b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
