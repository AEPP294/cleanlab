{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning import Trainer, seed_everything\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cleanlab import Datalab, filter, rank, dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_model: 975, 93\n",
    "# model1: 1700, 97\n",
    "# model2: 550, 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 48000\n",
      "Length of validation set: 12000\n",
      "Length of test set: 10000\n",
      "Example entry: (tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]),6)\n",
      "Shape: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Load data sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "# testloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Length of training set: {len(train_set)}\\nLength of validation set: {len(valid_set)}\\nLength of test set: {len(test_set)}\")\n",
    "print(f\"Example entry: ({train_set[0][0][0,0,:]},{train_set[0][1]})\\nShape: {train_set[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNIST' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/Work/cleanlab/issue_862.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/durdledoor/Library/Mobile%20Documents/com~apple~CloudDocs/Work/cleanlab/issue_862.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_set\u001b[39m.\u001b[39;49mdtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MNIST' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "test_set.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "           \n",
    "        # Define the CNN as a Sequential module\n",
    "        # self.conv_layer = nn.Conv2d(1, 2, kernel_size=4, padding=0)\n",
    "        # horizontal_edge_filter = torch.tensor([[1, 1, 1], [0, 0, 0], [-1, -1, -1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        # vertical_edge_filter = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "\n",
    "        # # Assign the filters to the convolutional layer\n",
    "        # self.conv_layer.weight.data[0, 0, :, :] = horizontal_edge_filter\n",
    "        # self.conv_layer.weight.data[1, 0, :, :] = vertical_edge_filter\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(3, 1, kernel_size=5, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2)\n",
    "        )\n",
    "        self.linear = nn.Sequential(nn.LazyLinear(16), nn.ReLU())\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(1, 1, kernel_size=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=6, stride=5)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def embeddings(self, x):\n",
    "        x = torch.flatten(self.cnn(x),1) + torch.flatten(self.residual(x),1)\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "           \n",
    "        self.conv_layer = nn.Conv2d(1, 3, kernel_size=2, padding=2)\n",
    "        horizontal_edge_filter = torch.tensor([[1, 1], [0, 0]], dtype=torch.float32).view(2,2)\n",
    "        vertical_edge_filter = torch.tensor([[1, 0], [1, 0]], dtype=torch.float32).view(2,2)\n",
    "        diagonal_edge_filter = torch.tensor([[1,0], [0,1]], dtype=torch.float32).view(2,2)\n",
    "\n",
    "        # Assign the filters to the convolutional layer\n",
    "        self.conv_layer.weight.data[0, :, :] = horizontal_edge_filter\n",
    "        self.conv_layer.weight.data[1, :, :] = vertical_edge_filter\n",
    "        self.conv_layer.weight.data[2, :, :] = diagonal_edge_filter\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            self.conv_layer,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(3, 1, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2)\n",
    "        )\n",
    "        self.linear = nn.Sequential(nn.LazyLinear(16), nn.ReLU())\n",
    "\n",
    "        self.residual = nn.Conv2d(1, 1, kernel_size=1, stride=4)\n",
    "        \n",
    "        self.output = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def embeddings(self, x):\n",
    "        # print(self.cnn(x).shape)\n",
    "        # print(self.residual(x).shape)\n",
    "        x = torch.flatten(self.cnn(x)+self.residual(x),1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, min_delta=10, patience=6):\n",
    "        self.min_delta=10\n",
    "        self.patience=6\n",
    "        self.min_loss=float('inf')\n",
    "        self.count=0\n",
    "    \n",
    "    def early_stop(self, loss):\n",
    "        if loss - 10 > self.min_loss:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.min_loss = loss\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 3, 31, 31]              15\n",
      "            Conv2d-2            [-1, 3, 31, 31]              15\n",
      "              ReLU-3            [-1, 3, 31, 31]               0\n",
      "         MaxPool2d-4            [-1, 3, 15, 15]               0\n",
      "            Conv2d-5            [-1, 1, 16, 16]              49\n",
      "              ReLU-6            [-1, 1, 16, 16]               0\n",
      "         MaxPool2d-7              [-1, 1, 7, 7]               0\n",
      "            Conv2d-8              [-1, 1, 7, 7]               2\n",
      "            Linear-9                   [-1, 16]             800\n",
      "             ReLU-10                   [-1, 16]               0\n",
      "           Linear-11                   [-1, 10]             170\n",
      "================================================================\n",
      "Total params: 1,051\n",
      "Trainable params: 1,051\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.08\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 3, 31, 31]              15\n",
      "            Conv2d-2            [-1, 3, 31, 31]              15\n",
      "              ReLU-3            [-1, 3, 31, 31]               0\n",
      "         MaxPool2d-4            [-1, 3, 15, 15]               0\n",
      "            Conv2d-5            [-1, 1, 16, 16]              49\n",
      "              ReLU-6            [-1, 1, 16, 16]               0\n",
      "         MaxPool2d-7              [-1, 1, 7, 7]               0\n",
      "            Conv2d-8              [-1, 1, 7, 7]               2\n",
      "            Linear-9                   [-1, 16]             800\n",
      "             ReLU-10                   [-1, 16]               0\n",
      "           Linear-11                   [-1, 10]             170\n",
      "================================================================\n",
      "Total params: 1,051\n",
      "Trainable params: 1,051\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.08\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "Epoch 0/32: Train loss: 695.544189453125\tVal loss: 5901.86865234375\n",
      "Epoch 1/32: Train loss: 300.1340026855469\tVal loss: 4595.26123046875\n",
      "Epoch 2/32: Train loss: 257.8421325683594\tVal loss: 3957.88720703125\n",
      "Epoch 3/32: Train loss: 236.29159545898438\tVal loss: 3795.725830078125\n",
      "Epoch 4/32: Train loss: 223.42698669433594\tVal loss: 3603.37060546875\n",
      "Epoch 5/32: Train loss: 216.26271057128906\tVal loss: 3473.0283203125\n",
      "Epoch 6/32: Train loss: 208.75912475585938\tVal loss: 3628.85546875\n",
      "Epoch 7/32: Train loss: 204.23068237304688\tVal loss: 3330.860595703125\n",
      "Epoch 8/32: Train loss: 199.2633819580078\tVal loss: 3618.56396484375\n",
      "Epoch 9/32: Train loss: 196.49147033691406\tVal loss: 3303.3505859375\n",
      "Epoch 10/32: Train loss: 190.87693786621094\tVal loss: 3185.978271484375\n",
      "Epoch 11/32: Train loss: 187.86988830566406\tVal loss: 3034.999755859375\n",
      "Epoch 12/32: Train loss: 184.32696533203125\tVal loss: 3360.69970703125\n",
      "Epoch 13/32: Train loss: 182.5767059326172\tVal loss: 2944.1806640625\n",
      "Epoch 14/32: Train loss: 177.86643981933594\tVal loss: 2897.593017578125\n",
      "Epoch 15/32: Train loss: 175.08480834960938\tVal loss: 2764.519775390625\n",
      "Epoch 16/32: Train loss: 171.99440002441406\tVal loss: 2840.94287109375\n",
      "Epoch 17/32: Train loss: 169.6221160888672\tVal loss: 2825.370849609375\n",
      "Epoch 18/32: Train loss: 169.5508575439453\tVal loss: 2736.31689453125\n",
      "Epoch 19/32: Train loss: 166.193115234375\tVal loss: 2743.001220703125\n",
      "Epoch 20/32: Train loss: 166.004638671875\tVal loss: 2728.10009765625\n",
      "Epoch 21/32: Train loss: 164.650390625\tVal loss: 2674.854248046875\n",
      "Epoch 22/32: Train loss: 161.73497009277344\tVal loss: 2667.857666015625\n",
      "Epoch 23/32: Train loss: 160.90025329589844\tVal loss: 2652.798583984375\n",
      "Epoch 24/32: Train loss: 158.8658447265625\tVal loss: 2707.764892578125\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "# skipping kfold\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 48\n",
    "n_epochs = 32\n",
    "\n",
    "patience = 8\n",
    "min_delta = 10\n",
    "\n",
    "model = Net()\n",
    "summary(model, (1,28,28))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    # start_epoch = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for _, batch in enumerate(trainloader):\n",
    "\n",
    "        xbatch, ybatch = batch\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(xbatch)\n",
    "        loss = criterion(outputs, ybatch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_acc = []\n",
    "        for _, batch in enumerate(valid_loader):\n",
    "            xbatch, ybatch = batch\n",
    "            \n",
    "            outputs = model(xbatch)\n",
    "            # y_hat = torch.argmax(torch.softmax(outputs,axis=1),axis=1)\n",
    "\n",
    "            val_loss += criterion(outputs, ybatch)\n",
    "            # val_acc.append(balanced_accuracy_score(ybatch, y_hat))\n",
    "\n",
    "    print(f'Epoch {epoch}/{n_epochs}: Train loss: {running_loss}\\tVal loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = []\n",
    "acc = []\n",
    "with torch.no_grad():\n",
    "    for xbatch, ybatch in testloader:\n",
    "        logits = model(xbatch)\n",
    "        proba = nn.Softmax(dim=1)(logits)\n",
    "        pred = torch.argmax(proba, dim=1)\n",
    "        score = balanced_accuracy_score(pred, ybatch)\n",
    "        balanced.append(score)\n",
    "        score = accuracy_score(pred, ybatch)\n",
    "        acc.append(score)\n",
    "    \n",
    "\n",
    "print(sum(balanced)/len(balanced))\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.logits = model(X)\n",
    "        self.proba = nn.Softmax(dim=1)(self.logits)\n",
    "        self.y_hat = torch.argmax(self.proba, dim=1)\n",
    "\n",
    "        return self.y_hat\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \n",
    "        self.predict(X)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy_score(y, self.y_hat)}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_accuracy_score(y, self.y_hat)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
